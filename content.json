{"meta":{"title":"Welcome to Phil's","subtitle":"At last, you are here! Don't forget to click the Ad above before you leave :p .","description":null,"author":"Phil Xu","url":"http://www.philsblog.info"},"pages":[{"title":"","date":"2018-03-27T06:14:26.000Z","updated":"2020-04-26T08:11:35.110Z","comments":true,"path":"about/index.html","permalink":"http://www.philsblog.info/about/index.html","excerpt":"","text":"Dear friend, If you’re reading this, you know me from somewhere.And if you’ve come this far, maybe you’re willing to come a little further. I am a Developer, prefer but not limit to use Java language, having 4+ years of programming experience.I am not just a Programmer, I’d love to solve problems, find answers, and learn new skills!I am passionate about my career, and wish devoute the next decade to this industry. You can click the tab above or the link below to know more about my work history, find out what I designed and created in the past few years! 2e Systems Financial Management Group State Grid Fiberlink Inc. Servian Pty Ltd Here, let’s see some specific material!!!!! My skill list! Skill detail Java CoreSpring frameworks/ EJBHibernateJSP JSF JMS JCSLog4j Quartz Tomcat Glassfish Second language CPythonJava script Theory Design patternAlgorithm Linux Linux commandShell script Spring Spring bootSpring cloudSpring dataSpring mvcSpring aop Tools NginX Web services SOAPRESTFUL Version Control gitmercurial Hg Q KafkaactiveMq Testing TDDJunit DB MySqlPostgresmongoDBH2 CI/CD JenkinsAWS codeBuild/codePipelineDocker IDE EclipseIntelliJ IDEANetBeans Config XML JSON YAML I believe cloud &amp; data are the future!!! and I am currently focusing on GCP data engineering, wish to get the cert soon.Feel free to talk to me if we share the same interests. Cheers,Phillip P.S. To know more about me, probably you want to visit my linkedin. Phillip xu"}],"posts":[{"title":"Rakuten Mobile","slug":"Rakuten-Mobile","date":"2020-03-16T07:41:20.000Z","updated":"2020-04-26T10:45:24.283Z","comments":true,"path":"2020/03/16/Rakuten-Mobile.html","link":"","permalink":"http://www.philsblog.info/2020/03/16/Rakuten-Mobile.html","excerpt":"","text":"Architect &amp; Automation EngineerMelbourne, Australia, 2019 - 2020 Description: Building and configuring security management solution on RedHat Enterprise Linux server in OpenStack.Responsibilities: Build and configure Vault cluster using Ansible DR setup, PR setup, auto unseal with HSM and GCP KMS Use Packer build images Use Vagrant setup local RHEL environment for OS patching Use Terraform Vault provider to create vault resources OS hardening using CIS benchmark. Integration with Jenkins pipeline. Configure network routing among the clusters. Telemetry Monitoring using statsD and Icinga, build customized server and script using node and bash. Documentation Tech Stack Ansible, Vault, Consul, Packer, Vagrant, statsD, Jenkins, RHEL, HSM, GCP KMS, bash, python, OpenStack, Linux","categories":[{"name":"Work history","slug":"Work-history","permalink":"http://www.philsblog.info/categories/Work-history/"},{"name":"Servian/Rakuten Mobile","slug":"Work-history/Servian-Rakuten-Mobile","permalink":"http://www.philsblog.info/categories/Work-history/Servian-Rakuten-Mobile/"}],"tags":[{"name":"Work history","slug":"Work-history","permalink":"http://www.philsblog.info/tags/Work-history/"}]},{"title":"Hashicorp Vault Basics","slug":"Hashicorp-Vault-Basics","date":"2019-10-18T03:35:47.000Z","updated":"2020-04-26T11:40:06.708Z","comments":true,"path":"2019/10/18/Hashicorp-Vault-Basics.html","link":"","permalink":"http://www.philsblog.info/2019/10/18/Hashicorp-Vault-Basics.html","excerpt":"","text":"Install Hashicorp vault on GCPThere is a existing module provided terraform-google-modules/vault/google Follow the instruction of this module, there are some minor issues though, not sure if it is fixed or not, I will provide how I install it here. add the following module to your main.tf 1234567891011module &quot;vault&quot; &#123; source = &quot;terraform-google-modules/vault/google&quot; project_id = var.project_id region = var.region kms_keyring = var.kms_keyring kms_crypto_key = var.kms_crypto_key&#125;output &quot;vault_addr&quot; &#123; value = module.vault.vault_addr&#125; tf apply export these two variable which will be picked up by vault, the name of the output should match the one defined in your main.tf 12$ export VAULT_ADDR=&quot;$(terraform output vault_address)&quot;$ export VAULT_CACERT=&quot;$(pwd)/ca.crt&quot; init your cluster, it basically shard a key into 5 pieces, to unseal the vault, you will need at least provide any 3 of the 5. Change the number to your liking, the default recommendation are 5 and 3. It will also generate the root token along with the unseal key. Save then somewhere safe. 123$ vault operator init \\ -recovery-shares 5 \\ -recovery-threshold 3 Most used commands1vault operator init -recovery-shares 5 -recovery-threshold 3 1vault login 1vault audit enable file file_path=/var/log/vault/audit.log 1vault secrets list -detailed 1234vault secrets enable -path=secret secretvault secrets enable -path=secret/ kvvault secrets enable kv-v2vault secrets enable -path=secret/ -version=2 kv 1vault kv enable-versioning secret/ 1vault secrets list -detailed 12345678910vault kv put secret/hello foo=worldvault kv put secret/apikey/splunk apikey=&quot;xxxxxx&quot;# $cat acme.json&#123;# &quot;organization&quot; : &quot;xxx&quot;,# &quot;region&quot;: &quot;bla&quot;,# &quot;zip_code&quot;: &quot;bla&quot;,# &quot;contact_email&quot;: &quot;bla&quot;# &#125;vault kv put secret/customer.acme @acme.jsonvault kv get -field=contact_email secret/customer/acme 1vault kv put secret/customer/acme contact_email=&quot;jennifer@acme.com&quot; put is a replacement, it’s not a merge. All the data will be overwriten after put, e.g. in the above json example, organization and region are lost if doing putit also keeps a different version1vault kv patch secret/customer/acme contact_email=&quot;jennifer@acme.com&quot; just update one field in the secret, use patch. No patch equivlant api yet, it is a wrap of many other command, do the hard work for you.1vault kv delete secret/apikey/splunk if using version 1, then it is deleted, if usinv version 2, then the secrets are just marked as deleted.if you want to really delete the secret in version 2, do a destroy1vault kv destroy secret/apikey/splunk 1vault kv etadata delete secret/apikey/splunk example of using apis1234567891011121314151617$curl --header &quot;X-Vault-Token:&lt;tken&gt;&quot; \\ --request POST \\ --data &apos;&#123; &quot;apikey&quot;: &quot;3230falksj2jsd&quot; &#125;&apos; https://vault.rocks/v1/secret/apikey/splunk$curl --header &quot;X-Vault-Token:&lt;tken&gt;&quot; \\ --request GET \\ https://vault.rocks/v1/secret/apikey/splunk$curl --header &quot;X-Vault-Token:&lt;tken&gt;&quot; \\ --request DELETE \\ https://vault.rocks/v1/secret/apikey/splunk# if you are using kv 2, then url will have a &quot;data&quot; in the middle$curl --header &quot;X-Vault-Token:&lt;tken&gt;&quot; \\ --request DELETE \\ https://vault.rocks/v1/secret/data/apikey/splunk CHeck and set, only for kv-v2tell the engine, alert the user when changing the secretenable it on the secret engine mounted at secret1vault write secret/config cas-required=true enable it only on the secret/partner path1vault kv metadata put -cas-required=true secret/partner when check and set is enabled, every write operation requres cas value, set the cas to 0 if the write should be allowed only if the key does not exist. Otherwise, set it to the current versin number.1vault kv put -cas=3 secret/apikey admin-key=&quot;fasjdfslkjf&quot; UI is actually nicer if you want to do versioning, in UI, you can create a new version based on a seleceted history version.1vault kv get secret/hello Cubbyhole used to store arbitrary secrets enabled by default at thecubbyhole/path Every little slot is dedicated to an individual user. Its lifetime is linked to the token used to write the data, no TTL in cubbyhole Even root cannot read the data if it wasn’t written by the root Cubbyhole secret engine cannot be disabled, moved or enabled multiple times vault write cubbyhole/github_token token=”1234121”vault read cubbyhole/github_token $curl –header “X-Vault-Token:“ –request POST \\ –data ‘{“token”: “1234643252342”}’ https://vault.rocks/v1/cubyhole/github_token response wrappingDynamic Secretsdynamic secrets are generated when they are accessed, they do not exist until they are read, so there is no risk of someone stealing them, or another client using the same secrets.","categories":[{"name":"Ops","slug":"Ops","permalink":"http://www.philsblog.info/categories/Ops/"},{"name":"Vault","slug":"Ops/Vault","permalink":"http://www.philsblog.info/categories/Ops/Vault/"}],"tags":[{"name":"Hashicorp","slug":"Hashicorp","permalink":"http://www.philsblog.info/tags/Hashicorp/"},{"name":"Vault","slug":"Vault","permalink":"http://www.philsblog.info/tags/Vault/"}]},{"title":"ANZ Insto","slug":"ANZ-Insto","date":"2019-09-26T08:28:28.000Z","updated":"2020-04-26T10:43:54.667Z","comments":true,"path":"2019/09/26/ANZ-Insto.html","link":"","permalink":"http://www.philsblog.info/2019/09/26/ANZ-Insto.html","excerpt":"","text":"Full-Stack &amp; SREMelbourne, Australia, 2018 - 2019 Description: Building a financial dashboard application and data analytics solution on GCP using .Net Core, React, Kubernetes and AirflowResponsibilities: Developing web application using React Micro-frontend architecture Building data pipeline using Airfow Building customed pipeline using .Net Core Building test cases using Jasmin, mocha, react-testing-library Building &amp; Maintaining GKE cluster using Terraform Building service mesh using Istio Building Gitlab, Jira, Confluence platform using helm tiller in cloud Building Gitlab CI/CD pipeline Metrics Monitoring using Prometheus and Grafana Tech Stack: .Net Core, React, JavaScript, Kubernetes GKE, GCP, Airflow, Istio, Gitlab, Prometheus, Grafana, Stack Driver, Terraform, Jasmin, Mocha, react-testing-libary, helm/tiller","categories":[{"name":"Work history","slug":"Work-history","permalink":"http://www.philsblog.info/categories/Work-history/"},{"name":"Servian/ANZ","slug":"Work-history/Servian-ANZ","permalink":"http://www.philsblog.info/categories/Work-history/Servian-ANZ/"}],"tags":[{"name":"Work history","slug":"Work-history","permalink":"http://www.philsblog.info/tags/Work-history/"}]},{"title":"Common GCP command","slug":"Common-GCP-command","date":"2018-07-17T23:28:59.000Z","updated":"2020-04-26T11:31:48.254Z","comments":true,"path":"2018/07/18/Common-GCP-command.html","link":"","permalink":"http://www.philsblog.info/2018/07/18/Common-GCP-command.html","excerpt":"","text":"GCP commands: app enginegcloud app createmvn appengine:deploy choose projectgcloud config listgcloud config list accountgcloud config list project gcloud config set project gcp-test-xxxxxgcloud projects list Authentication &amp; organizationsgcloud auth logingcloud auth listgcloud organizations list","categories":[{"name":"Cloud","slug":"Cloud","permalink":"http://www.philsblog.info/categories/Cloud/"},{"name":"GCP","slug":"Cloud/GCP","permalink":"http://www.philsblog.info/categories/Cloud/GCP/"},{"name":"Commands","slug":"Cloud/GCP/Commands","permalink":"http://www.philsblog.info/categories/Cloud/GCP/Commands/"}],"tags":[{"name":"GCP","slug":"GCP","permalink":"http://www.philsblog.info/tags/GCP/"}]},{"title":"Cloud ML Engine & Machine Learning & ML API","slug":"Cloud-ML-Engine","date":"2018-07-04T00:09:54.000Z","updated":"2020-04-26T11:26:43.197Z","comments":true,"path":"2018/07/04/Cloud-ML-Engine.html","link":"","permalink":"http://www.philsblog.info/2018/07/04/Cloud-ML-Engine.html","excerpt":"","text":"Use python to run c++ Scale tiers BASIC STANDARD_1 PREMIUM_1 BASIC_GPU BASIC_TPU CUSTOM On GCP:Collect data login APIs cloud pub-sub other real-time streaming Organize data BigQuery Dataflow Machine learning processing SDK Create model Tensorflow Train &amp; deploy model Cloud engine TensorFlow categorical_column_with_hash_bucketUse it when we don’t know the set of possible values in advance, each possible value in the feature column occupation will be hashed to an integer ID as we encounter them in training. categorical_column_with_identityUse this when your inputs are integers in the range [0, num_buckets), and you want to use the input value itself as the categorical ID. Values outside this range will result in default_value if specified, otherwise it will fail. categorical_column_with_vocabulary_file/listUse it when you know the set of all possible feature values of a column and there are only a few of them Question:tf.session() Wide &amp; Deep learningWide for memorization: seagulls can fly, pigeons can flyDeep for generalization: animals with wings can flyOne can combine the strengths of both.It’s useful for generic large-scale regression and classification problems with sparse inputs (categorical features with a large number of possible feature values), such as recommender systems, search, and ranking problems. Run a ML engine training job locally1gcloud ml-engine local train This command runs the specified module in an environment similar to that of a live Cloud ML Engine Training Job.This is especially useful in the case of testing distributed models, as it allows you to validate that you are properly interacting with the Cloud ML Engine cluster configuration. Type of MLhttps://elitedatascience.com/machine-learning-algorithms RegressorSuoervised learning task for modeling and predicting contiunuous, numeric variables. such as prediciting real=estate preices, stock priuces movements or student test scores. ClassifierClassification is supervised learning task for modeling and predicting categorical variables. Such as predicting employee churn, email spam, financial fraud, or student letter grades. Clustering estimatorClustering is an unsupervised learning task for finding natural groupings of observations based on the inherent structure within your dataset. Such as customer segmentation, grouping similar items in e-commerce, and social network analysis. Supervised learning vs Unsupervised learning test cases with label or not. Online prediction vs batch prediction OnlineOptimized to minimize the latency of serving predictions.Predictions returned in the reponse message. BatchOptimized to handle a high volume of instances in a job and to run more complex models.Predictions written to output files in a Cloud Storage location that you specify.https://cloud.google.com/ml-engine/docs/tensorflow/prediction-overview#online_prediction_versus_batch_prediction One-hot columnhttps://hackernoon.com/what-is-one-hot-encoding-why-and-when-do-you-have-to-use-it-e3c6186d008f Embeddinghttps://www.tensorflow.org/programmers_guide/embedding HyperparametersIf model parameters are variables that get adjusted by training with existing data, your hyperparameters are the variables about the training process itself. Number of hidden layers Number of nodes in each hidden layer Learning rate Feature engineering base feature column - one of the raw columns in the original dataframe derived feature column - any new columns created based on some transformations defined over one or multiple base columns Type: Bucketization Crossed feature columns You could use bucketization to turn year of birth and income into categorical features, but thr raw conlumns are continuous. Error measurementRegression: MSEClassification: cross-entropy Performance evaluationFor unbalanced datasetconfusion matrix - to describe the performance Accuracy when ML says “cat” —— 991 / 1000 - if balancePrecision = TP / (FP + FP) —— 1/1 - if unbalanced - accuracy when classifier says yes - things are commonRecall = TP / (TP + FN) —— 1/10- if unbalanced - Accuracy when the truth is yes - things are rare Prepare the dataset:Good dataset should cover all cases, Ensure to include negative casesBut before you throw out the outliers, you need to first see whether if you can collect enough outliers, for some outliers: cannot just throw out it a better approach: why does this happen? reflect something about the data You want to make sure enough examples in the training dataset, so should keep it the fewer things you throw out, the more robust your model is splitting datasetOriginal data split into:training datavalidation dataTest data cross validation - change the split, take the average, if data is not enough ML APIsNatural language API analyses Sentiment Category Entity Syntax Machine learningOverfittingIf our model does much better on the training set than on the test set, then we’re likely overfitting. Reduce number of nodes or layersDropout or early stopping bias and variance MapReduce vs SparkSpark in memory, fast, 100 times faster than hadoop mapreduce, good for streaming Resilient Distributed Datasets can use disk near real-time analytics MapReduce Persistent storage Question:what is TextLineReader - read directly to graphbucketized_column - discretizing a continuous variablesparse_column_with_keys - encoding categorical data","categories":[{"name":"Cloud","slug":"Cloud","permalink":"http://www.philsblog.info/categories/Cloud/"},{"name":"GCP","slug":"Cloud/GCP","permalink":"http://www.philsblog.info/categories/Cloud/GCP/"},{"name":"Cloud ML Engine","slug":"Cloud/GCP/Cloud-ML-Engine","permalink":"http://www.philsblog.info/categories/Cloud/GCP/Cloud-ML-Engine/"}],"tags":[{"name":"GCP","slug":"GCP","permalink":"http://www.philsblog.info/tags/GCP/"},{"name":"ML","slug":"ML","permalink":"http://www.philsblog.info/tags/ML/"},{"name":"Cloud ML Engine","slug":"Cloud-ML-Engine","permalink":"http://www.philsblog.info/tags/Cloud-ML-Engine/"}]},{"title":"BigTable","slug":"BigTable","date":"2018-07-02T12:34:22.000Z","updated":"2020-04-26T11:27:55.193Z","comments":true,"path":"2018/07/02/BigTable.html","link":"","permalink":"http://www.philsblog.info/2018/07/02/BigTable.html","excerpt":"","text":"feature summaryIt’s OLAP, so no transaction.More expensive than bigqeuryLow latency, high throughput100,000 QPS @ 6ms latency for 10-node clusterYou pay for the number of clusterGlobal availabilityAlso separate computing and storage - the cluster only contains the pointer to the data, the data is remain on colossus, nodes only hold meta dataData is stored on contiguous rows, as tablets, stored on GCSCreate the cluster first. You can add nodes, remove nodes without having any down timeUse Python or data flow, HBase client, HBase API, BigQueryInsert data by creating a mutationRequests -&gt; clients -&gt; front-end server -&gt; nodesSwitching between SSD and HDD storageGood for ML continues trainingYou can use Cloud Dataproc to create one or more Compute Engine instances that can connect to a Cloud Bigtable instance and run Hadoop jobs.Bigtable tables are sparse; if a cell does not contain any data, it does not take up any space. Support 4 dimention data model: rowkey, column family, column, timesstampDoes this mean Bigtable support SCD? Instance type: Production/Development, difference?Prod - minimun 3 nodesDev - one single-node cluster0.65/hr per node for both types rowKey sorted in a Ascding order unique id can be premitive, structure or array represent internally as byte array columns can be added on the fly, however the column family cannot Read &amp; writestructureOnly one rowkey, none of the other columes can be indexedColumn family - related columns that tends to be updated together design Return adjacent rows as much as possible distributed writing as much as possible distributed reads as much as possible Avoid hotspotting Field promotion - prefered avoids hotspotting in almost all cases and it tends to make it easier to desing a row key that facilitates queries Salting Wide short vs narrow tallWide for dense dataNarrow for sparse data, e.g. user &amp; followerThe row key should consist of the the most queried data Query the data using: Row key Row prefix Row range Rows are all sorted in ascending order. - from the lowest to the highestContiguous rowkeys will be stored on the same tablet, which make is quicker for searching. （you want the related entities are stored adjacently）If you need the last few recored - then use reverse timestamp Avoid Keyrow type examples : domains sequential IDs static, repeatedly updated identifiers Best practiceBigTable learns over time - have to leave BigTable up and running in a typical scenario. Let big query warm up for couple of hours and make sure you are not using BigTable for small amount of data(cannot be smaller than 300GB, recommended greater than 1T)BigTable will re-balance the data - which allows imperfect row key design performanceshould expect 10000 query/ second at about 6 millisecond latency read and write on SSD 10000QPS 50 latency writing on HDD 500QPS at 200ms latency on HDD The number of nodes is linearly related to performancemake sure the client and BigTable are in the same zone. PricePay for the nodes per hourand the storage volumne used HDD vs SSDSSD clusters: 2.5 TB per nodeHDD clusters: 8 TB per node","categories":[{"name":"Cloud","slug":"Cloud","permalink":"http://www.philsblog.info/categories/Cloud/"},{"name":"GCP","slug":"Cloud/GCP","permalink":"http://www.philsblog.info/categories/Cloud/GCP/"},{"name":"BigTable","slug":"Cloud/GCP/BigTable","permalink":"http://www.philsblog.info/categories/Cloud/GCP/BigTable/"}],"tags":[{"name":"GCP","slug":"GCP","permalink":"http://www.philsblog.info/tags/GCP/"},{"name":"BigTable","slug":"BigTable","permalink":"http://www.philsblog.info/tags/BigTable/"}]},{"title":"BigQuery","slug":"BigQuery","date":"2018-07-01T04:24:58.000Z","updated":"2020-04-26T11:28:00.719Z","comments":true,"path":"2018/07/01/BigQuery.html","link":"","permalink":"http://www.philsblog.info/2018/07/01/BigQuery.html","excerpt":"","text":"feature summary Petabyte scale DW on GCP for interactive analysis (near real time analysis, it’s not totally real time, cannot respond in millisecond, microsecond). Prefer demoralised table structure using nested, repeated fields. Separate storage and compute. Data stored in bigQuery is durable, it’s replicated in multiple places and it’s pretty inexpensive in terms of storing. Support both standard SQL(SQL 2011), legacy SQL. Default is legacy SQL. Standard SQL is preferred since BigQuery 2.0 No-ops Generate immutable audit logs. you know every time somebody’s runs a query on it and you basically get those audit logs which cannot be tampered with. So you will know what people have done to each dataset. CacheQuery results are cached for approximately 24 hours by defaultNot cached cases: When a destination table is specified in the job configuration, the web UI, the command line, or the API If any of the referenced tables or logical views have changed since the results were previously cached When any of the tables referenced by the query have recently received streaming inserts (a streaming buffer is attached to the table) even if no new rows have arrived If the query uses non-deterministic functions; for example, date and time functions such as CURRENT_TIMESTAMP() and NOW(), and other functions such as CURRENT_USER() return different values depending on when a query is executed If you are querying multiple tables using a wildcard If the cached results have expired; typical cache lifetime is 24 hours, but the cached results are best-effort and may be invalidated sooner If the query runs against an external data source Structure Big query is not transactional every column is stored in a separate file which is encrypted and replicated BQ is primarily meant for immutable very large datasets table is materialised / view is live Tools with connector:Tableau, Looker, Qlik, Data studio, etc Supported ingest format: JSON (newline delimited only), CSV, Avro, Parquet, ORC, Google Cloud Datastore (Store on GCS only). Ingestion:streaming and batchWhy use dataflow if we can use stream input directly on BigQuery? Because BigQuery only accept table format, the source data mey need to be converted in dataflow.Can load data from: File upload Google Cloud Storage Google Drive Google Cloud Bigtable Stream data in from Fluentd by using a plugin Cannot load from:Cloud SQL Cannot use the Web UI to: Upload a file greater than 10 MB in size Upload multiple files at the same time Uploada file in SQL format All the above three operations can be performed using the “bq” command. SharingBigQuery is a way for you to share data beyond the silos of your company’s structure, it’s a way of collaborationyou can share the dataset, allow them to write ad hoc query, also share the query, share the ability of analysisaccess control on data setdataSets contain tables and viewsYou can select specific rows and columns and save it as view, then share it with others(other project) External storage:GCS, google sheet, BigTable From BigTablePermanent - can share Create a table definition file (for the CLI or API) Create a table in BigQuery linked to the external data source Query the data using the permanent table To share you need: READER or bigquery.dataViewer biguqery.user bigtable.reader Temporaryit cannot be shared with others, is useful for one-time, d-hoc queries over external data, or for extract, transform, and load (ETL) processes. A table definition file with a query An inline schema definition with a query A JSON schema definition file with a query Create a table definition file Submit both a query and a table definition file SQL Functions:ARRAY_AGGUNNESTREGEXP_CONTAINSSTARTS_WITH NavigationLEAD, LAG, NTH_VALUE Window functionRANK OVER approx function tipsonly order at outer queryuse skydiver GUI to monitor bigquery PricingPay for using / flat rate priceCancel jobs, pay for what have processed so far.Can Set billing limits Charge on ingest streaming1TB/month free$5 per TBDiscount on old data.Storage cost is about the same as the cost of GCS, get the sam discount as well (after 90 days) can limit the usage on the project or the user reduce cost Use preview options Use Query validator before you run your query Avoiding select * statement Hard limit the capacity allowed to be processed on projects and users PartitionUp to 2,500 partitionsDaily limits: 2,000 partition updates per table per dayRate limit: 50 partition updates every 10 secondsYou cannot change an existing table into a partitioned table. You must create a partitioned table from scratch. Then you can either stream data into it every day and the data will automatically be put in the right partition, or you can load data into a specific partition by using “$YYYYMMDD” at the end of the table name. pseudo column _PARTITIONTIMEuser can use pseudo column, but cannot see it wildcardunion join all similar named tables within the datasettables must have the same structure_TABLE_SUFFIX in where clause Authorized Views:Create views, add account to the view, add authorized view from the source tableAn authorized view allows you to share query results with particular users and groups without giving them read access to the underlying tables. Authorized views can only be created in a dataset that does not contain the tables queried by the view. When you create an authorized view, you use the view’s SQL query to restrict access to only the reows and columns you want the users to see. Questions:Is could SQL and Spanner respond quicker than BigQuery?what’s tail latency? tail skew?use having clause to avoid tail skew Jobs load extract query copy IAMFor example, a user who merely has bigquery.dataViewer permissions on a dataset without any other permissions can only list the tables in the dataset and use the get() APIs to read the contents of the tables. The user cannot query the data without additional permissions. For an example of a user with permissions to run queries, consider the following scenario. A user who has bigquery.user permissions in projectA, and bigquery.dataViewer permissions on projectA:dataset1 and projectB:dataset2 can run a query in projectA that uses either or both of these datasets. The dataViewer doesn’t have the ability to incur costs Roles bigquery.user Permissions to run jobs, including queries, within the project. Enumerate their own jobs, cancel their own jobs, and enumerate datasets within a project. Allows the creation of new datasets within the project; the creator is granted the bigquery.dataOwner role for these new datasets. bigquery.jobUser Permissions to run jobs, including queries, within the project. Can enumerate their own jobs and cancel their own jobs. Does not allow access to any BigQuery data bigquery.dataViewer Read the dataset’s metadata and to list tables in the dataset. Read data and metadata from the dataset’s tables. bigquery.dataEditor Read the dataset’s metadata and to list tables in the dataset. Create, update, get, and delete the dataset’s tables. bigquery.dataOwner Read, update, and delete the dataset. Create, update, get, and delete the dataset’s tables. bigquery.admin all Appliction profile?? stores settings that tell your Cloud Bigtable instance how to handle incoming requests from an application affect how your applications communicate with an instance that uses replication especially useful for instances that have 2 clusters","categories":[{"name":"Cloud","slug":"Cloud","permalink":"http://www.philsblog.info/categories/Cloud/"},{"name":"GCP","slug":"Cloud/GCP","permalink":"http://www.philsblog.info/categories/Cloud/GCP/"},{"name":"BigQuery","slug":"Cloud/GCP/BigQuery","permalink":"http://www.philsblog.info/categories/Cloud/GCP/BigQuery/"}],"tags":[{"name":"GCP","slug":"GCP","permalink":"http://www.philsblog.info/tags/GCP/"},{"name":"BigQuery","slug":"BigQuery","permalink":"http://www.philsblog.info/tags/BigQuery/"}]},{"title":"Cloud Dataproc","slug":"Cloud-Dataproc","date":"2018-06-30T23:06:11.000Z","updated":"2020-04-26T11:26:38.615Z","comments":true,"path":"2018/07/01/Cloud-Dataproc.html","link":"","permalink":"http://www.philsblog.info/2018/07/01/Cloud-Dataproc.html","excerpt":"","text":"Dataproc takes care of all the over head and normally take 90 secs to spin upCheep storage Terabytes to Petabytes is good running it on cloud.Use CLI and GC console to operate. The console and GC command make the same API call to operate.Create clusters specifically for the jobSelect zone next to your GCS why GCS? Don’t need to specify the storage size if store it on GCS moving HDFS to GSC allows you to use fewer persistent workers and more pre-emptable workers, HDFS cannot be run on pre-eemptable worker HDFS replicate 3 times to prevent nodes failure, no need to worry about it if store on GCS. move to GCS: Copy local data to GCS Update file prefix from hfs:// to gs:// Use Cloud Dataproc Integration with GC storages Can dataproc read data from bigtable? bigquery?(via GCS, and big query connector, The connector to GCS is built into Dataproc) You can use Cloud Dataproc to create one or more Compute Engine instances that can connect to a Cloud Bigtable instance and run Hadoop jobs. After you create your Cloud Dataproc cluster, you can use the cluster to run Hadoop jobs that read and write data to and from Cloud Bigtable. You can use a BigQuery connector to enable programmatic read/write access to BigQuery. standard mode vs high availability mode? What type of jobs can I run?Cloud Dataproc provides out-of-the box and end-to-end support for many of the most popular job types, including Spark, Spark SQL, PySpark, MapReduce, Hive, and Pig jobs. Scaling Vertical scaling: larger computer horizontal scaling: more computers horizontal is better but difficult Migrate your hadoopIf your hadoop :reads and writes to a HBase, you can use bigTableHDFS -&gt; move to GCSIf read &amp; write SQL -&gt; bigQuery Spark &amp; pandaSpark is declarative programming, the opposite one is called imperative programmingYou tell the system what you want and it figures out how to implement ituse on-premise, cannot just add cluster, you need to rebalance the data resilient distributed data sets RDDs - hide the complexity of the location of data within the cluster and also the complexity of the replication.Spark partitions data and memory across the cluster and knows how to recover through an RTDD’s lineage should anything go wrong. panda format is in memory, limited by size. Don’t use it as full-data, use it after aggregationpanda dataframe is mutabel, spark dataframe is immutable. Initialization actions (was called installation scripts in the coursera course)you can load multiple initialisation scripts to customise the software on dataproc worker and masterDataproc is aware of new instances joining the cluster and make sure that they run the cluster’s initialisation scriptsmay need to wait a few minutes after the cluster spin upThe scripts are designed for all nodes(including master and worker), you can specify the package to be installed for different ROLEs in the scriptsdataproc install datalab as default? Hadoop Eco systemOozieOozie is a workflow scheduler system to manage Apache Hadoop jobs. SqoopTransfers large amounts of data into hfs from relational databases such as MySQL. It is a transferring framework. PigPig is a high level scripting language that is used with Apache Hadoop. Pig enables data workers to write complex data transformations without knowing Java. Pig’s simple SQL-like scripting language is called Pig Latin, and appeals to developers already familiar with scripting languages and SQL. HiveApache Hive is an open-source data warehouse system for querying and analyzing large datasets stored in HDFS.Hive supports HiveQL which is like SQL, HiveQL is then translated to map-reduce jobs in the background.Map-reduce is a data processing technique used commonly to extract, transform and load data on Hadoop Pre-emptible VMs Processing only No preemptible-only clusters Persistent disk is used for local caching and not available through HDFS The managed group automatically re-add workers lost due to reclamation PropertiesUpdate configuration files:12--properties file_prefix:property=value--properties 'spark:spark.master=spark://example.com'","categories":[{"name":"Cloud","slug":"Cloud","permalink":"http://www.philsblog.info/categories/Cloud/"},{"name":"GCP","slug":"Cloud/GCP","permalink":"http://www.philsblog.info/categories/Cloud/GCP/"},{"name":"Dataproc","slug":"Cloud/GCP/Dataproc","permalink":"http://www.philsblog.info/categories/Cloud/GCP/Dataproc/"}],"tags":[{"name":"GCP","slug":"GCP","permalink":"http://www.philsblog.info/tags/GCP/"},{"name":"Dataproc","slug":"Dataproc","permalink":"http://www.philsblog.info/tags/Dataproc/"}]},{"title":"Cloud Dataflow","slug":"Cloud-Dataflow","date":"2018-06-30T04:54:28.000Z","updated":"2020-04-26T11:27:49.370Z","comments":true,"path":"2018/06/30/Cloud-Dataflow.html","link":"","permalink":"http://www.philsblog.info/2018/06/30/Cloud-Dataflow.html","excerpt":"","text":"Dataflow TextIO.write will shard your file, you can avoid it by withoutSharding() Dataflow can do everything you do in MapReduceParDo: parallel processingshould not contain any stateProcess one item at a timecan do filteringExtracting parts of inputcalculating values from different parts of inputs Map Vs FlatMapMap: one input -&gt; one outputFlatMap: to filter, only return the value needed Combine Vs Group Bycombine by key is faster and optimised by googleGroup by is not balanced Major components Pipelines PCollection Transforms I/O Sources and Sinks Cancel jobs Cancelcancel it right away DrainDataflow service will immediately stop ingesting new data from input sources, but the Dataflow service will preserve any existing resources (such as worker instances) to finish processing and writing any buffered data in your pipeline. The Cloud Dataflow connectorSpannerBigTable - need a separate lib to connect to Bigtable, BigQuery doesn’t need it, it’s built in the apache beam lib Dataflow has built in R/W for BigQuery Windowingdefault windowing behavior is to assign all elements of a PCollection to a single global window, even for unbounded PCollection Fixed Sliding session single global (default) TriggersType: Time-based triggers The default trigger is time based, it emit after the watermark passes the end of the window AfterWatermark AfterProcessingTime Data-driven triggers Composite triggers Access Control The Cloud Dataflow Worker role (roles/dataflow.worker) provides the permissions (dataflow.workItems.lease, dataflow.workItems.update, and dataflow.workItems.sendMessage) necessary for a Compute Engine service account to execute work units for a Cloud Dataflow pipeline. It should typically only be assigned to such an account, and only includes the ability to request and update work from the Cloud Dataflow service. PCollectionIt seems the PCollection is immutable? Not sure, encountered an exception while playting with the code","categories":[{"name":"Cloud","slug":"Cloud","permalink":"http://www.philsblog.info/categories/Cloud/"},{"name":"GCP","slug":"Cloud/GCP","permalink":"http://www.philsblog.info/categories/Cloud/GCP/"},{"name":"Dataflow","slug":"Cloud/GCP/Dataflow","permalink":"http://www.philsblog.info/categories/Cloud/GCP/Dataflow/"}],"tags":[{"name":"GCP","slug":"GCP","permalink":"http://www.philsblog.info/tags/GCP/"},{"name":"Dataflow","slug":"Dataflow","permalink":"http://www.philsblog.info/tags/Dataflow/"}]},{"title":"Cloud Dataprep","slug":"Cloud-Dataprep","date":"2018-06-28T21:52:10.000Z","updated":"2020-04-26T11:26:48.712Z","comments":true,"path":"2018/06/29/Cloud-Dataprep.html","link":"","permalink":"http://www.philsblog.info/2018/06/29/Cloud-Dataprep.html","excerpt":"","text":"Interactive graphical system for preparing structured or unstructured data for use in:analytics -&gt; BigQueryvisualisation -&gt; Data Studiotrain machine learning models -&gt; Input integration : GCS, BigQuery and filesDataprep offers a graphical user interface for interactively designing a pipelineThe elements are divided in to datasets, recipes and output A dataset roughly translates into a Dataflow pipeline read,a recipe usually translates into multiple pipeline transformations.and output translates into a pipeline action create Dataflow pipelines without codingthe pipeline can be output as a Dataflow Template for continued use in Dataflow","categories":[{"name":"Cloud","slug":"Cloud","permalink":"http://www.philsblog.info/categories/Cloud/"},{"name":"GCP","slug":"Cloud/GCP","permalink":"http://www.philsblog.info/categories/Cloud/GCP/"},{"name":"Dataprep","slug":"Cloud/GCP/Dataprep","permalink":"http://www.philsblog.info/categories/Cloud/GCP/Dataprep/"}],"tags":[{"name":"GCP","slug":"GCP","permalink":"http://www.philsblog.info/tags/GCP/"},{"name":"Dataprep","slug":"Dataprep","permalink":"http://www.philsblog.info/tags/Dataprep/"}]},{"title":"Cloud Pub/Sub","slug":"Cloud-Pub-Sub","date":"2018-06-28T08:14:22.000Z","updated":"2020-04-26T11:26:46.877Z","comments":true,"path":"2018/06/28/Cloud-Pub-Sub.html","link":"","permalink":"http://www.philsblog.info/2018/06/28/Cloud-Pub-Sub.html","excerpt":"","text":"feature summaryMessage persist for 7 dayslow-lantencyCapturing data and distributing dataUnified global server less service - not attached to a specific project, domain or userSmooth out traffic spikes or bursty communicationsBalancing the messagesAutoscales to deal with variable volumes of dataSimplifies the distribution of eventsDecouple the publisher and subscriberNot just streaming, also support batch??? A subscriber ACKs each message for every subscriptionA message is resent if subscriber takes more than “ackDeadline” to respondA subscriber can extends the deadline per message Order is not guaranteedSystem need to deal with “out of order” and “possible duplication”Dataflow can handle both, by order and window? Fan in / Fan out PushInitiate a request to the subscriber application to deliver messages.The subscriber needs to be an HTTPS web application, needs to have a web hook that is accessible via HTTPS, needs to be a server applicationNo delay - ideal if you want low latency, immediate processing of messages, close to real time performanceIf cannot reach the subscriber - do a reduction exponential backoff- tries, increase the delay and try again. You can control the retry, it will retry for seven days. pullThe subscriber ask pubsubJust need to make a callThere will be a delay, because the subscriber is check the periodically.If good for if you have lots of subscribers, and those subscribers are dymaticlly created.The subscriber explicitly request the delivery of message that’s in the subscription queue, it return error if the queue is empty. And it reposed with ackID. Dataflow can deduplicate by a provided IDCan define your own timestamp for watermark Bigquery streaming ingestion:100 k rows / per table per second Dataflow running all the time, BigQuery for ad hoc","categories":[{"name":"Cloud","slug":"Cloud","permalink":"http://www.philsblog.info/categories/Cloud/"},{"name":"GCP","slug":"Cloud/GCP","permalink":"http://www.philsblog.info/categories/Cloud/GCP/"},{"name":"pubsub","slug":"Cloud/GCP/pubsub","permalink":"http://www.philsblog.info/categories/Cloud/GCP/pubsub/"}],"tags":[{"name":"GCP","slug":"GCP","permalink":"http://www.philsblog.info/tags/GCP/"},{"name":"pubsub","slug":"pubsub","permalink":"http://www.philsblog.info/tags/pubsub/"}]},{"title":"GCP Data Engineer preparation","slug":"GCP-Fundamentals","date":"2018-06-26T00:07:07.000Z","updated":"2020-04-26T11:31:24.194Z","comments":true,"path":"2018/06/26/GCP-Fundamentals.html","link":"","permalink":"http://www.philsblog.info/2018/06/26/GCP-Fundamentals.html","excerpt":"","text":"Here is a summarization of what I have learned so far on GCP data engineering. It includes all of the resources I found, such as: Coursera Google Data Engineer courses Udemy courses Google Cloud Platform Documentation Sample Exam Google Professional Data Engineer Dumps I know that you can find everything on Google Cloud Documents, here is a sub set of GCP document for the Data Engineer exam. In this page, most of the contents are bullet points. Have a quick look through them, see if you can tell the story behind each of the item. If you can, then you have a pretty good coverage for the GCP Data Engineer Cert. If you haven’t heard of them, don’t worry, you still have time, just google them or go to the sub page to find out more dertails. (There are some internal link which will give you a bit more information of the things summarized in this page.)Please just comment below if you found something missing from the exam and good luck! Why GCP? Why Cloud? MapReduce tends to be limited by the number of compute nodes that you have. Use cloud, you have: Grate scalability and reliability As no-ops as possible, minimise a system administration overhead As flexible as possible GCS Global scale data and computer infrastrure, Staging area BLOB, raw data in any format Disk goes away with compute engine, GCS will always be there durable, replicated can use it to stage the data onto other GCP products Load data to GCS: gustil cp K V pare, even it looks like directory. can get access by: REST API, console Transfer service - could be one time, could be recurring, only into GCS, not out source: locamachine, AWS S3, etc. every bucket belongs to a project. edge cached Each zone is a physical data center -zone a b and c doesn’t means the geographical zone.- to make sure all your vms are at the same place, do deploy them in the same session Highly durable, stores data redundantly, highly available, infinite scalability and consistent. Mainly used for storing binary or object data. Such as images, audio, video and backups. Cloud storage offers different products for accessibility and archival. Multi-regional - Ideal for content that is required to be served across geographic regions. Regional - For use within a single region. Nearline - Used for data accessed less than once a month. (Archival solution, you may still be able to retrieve in less then a second) Coldline - Used for data access less than once a year (Archival solution, you may still be able to retrieve in less then a second) Amazon Glacier may take hours Google Computer Engine (GCE) N1-standard-4 if only 60% of usage, get 15% discount preemptible VM 80% discount (Not sure) others can take it away anytime It’s useful when you just want to run an ad hoc function that is not incorporated in to other GCP Paas products. An instance can have only one service account. Cloud SQL Single machine Handle gigabytes of data Relational database, support update and deletes individual fields Flexible pricing, passivate it when not running, good for testing Managed backups by google connect from anywhere automatic replication fast connection from GCE &amp; GAE etc. Google security Support both compressed and uncompressed SQL dump files Dataproc Cluster contains master(s), workers Resizable Think the cluster as a job specific resources. Google managed hadoop Pig, Hive, Spark programs Hadoop ecosystem Spark - interactive SQL pig - ETL script Hive - do queries Presto??? storage Can use HDFS on clusters node. Can store data on GCS and use preemptible(probably 10 standard, 30 preemptible) to run your job.(You don’t want to store it on node, because you don’t want to pay the compute when you only need storage) separate the compute and storage. Store it in the save place as the cluster. Datastore Handle terabytes of data HashMap K V pare Search by key or property Transactional what is “kind” in datastore??? Support CRUD operation hierarchical data Highly scalable and managed NoSQL database. Highly Durable and Available. Supports ACID transactions (which is typical for most SQL data warehouse solutions), SQL-like queries. Cloud datastore are mainly used for User profiles, such as social network profiles and game statuses. Highly Scalable, automatic sharing and replication (Highly Available) Supports ACID Transactions (consistent and durable) SQL like queries Can Update Attributes (or add Attributes which is like the columns and rows) Time consum depends on the size of the result, not the size of the dataset BigTable Petabytes Support append only operation No transactional support Always right a new row, append to the table, read the latest data from the table backward, so always get the newest version of the object High throughput No-ops IoT flattened data Search only based on the rowkey, important to design the good key to find the key quickly, avoid hotspot, want it to be distributed. Make the table tall and narrow. column family, group related columns? but why????? BigQuery standard SQL 2011 / lagecy SQL user defined function in JS Query CSV, JSON, Avro files on GCS or Google sheets without ingestion, called federated data You can join data between google sheet and big query, can you join other type of data from GCS？ Loading data from GCS, can you load from Datastore? Can stream data in using dataflow as well. Datalabpython numpy pandaCan use Python SQL JS to process data DataStudio data sources: BigQuery, MySQL, CloudSQL, Google Sheet, Google analytics need the right permission live update Datastudio is stored on google drive dimension X metrics Y Cache: Query cache, Prefetch cache ML gradient descent - walk down an error surface - doesn’t guaranteed to converge learning rate - hyper parameter epoch - a traversal through the entire training dataset Weights - parameters we optimize Batch size - the amount of data we compute error on Evaluation - is the model good enough? Has to be done on full dataset Training - gradient descent + evaluation Neurons - one unit of combining inputs Hidden layer - set of neutrons that operate on the same set of inputs Inputs - What you feed into a neuron Features - transformation of inputs, such as x^2 Feature Engineering - coming up with what transformations to include Softmax - normalize all input probabilities Anything in ML model must be numeric TensorFlow Python -&gt; c++ StackDriver functions Debugger - allows you to inspect the state of a running application in real time without stopping or slowing it down. You can use it to see your code behavior in production. Error reporting - counts, analyzes and aggregates the crashes in your cloud services. You can see the errors in a management interface provided by Stackdriver. You can also receive emails and mobile alerts on new errors. Monitoring - provides an overview of performance, uptime and health of your cloud services. It collects metrics, events and metadata from GCP, AWS and other popular open source softwares. It ingests these metrics and displays them via a dashboard. Alerting - allows you to create policies to notify you when the health and uptime check results go over a certain limit. For example if uptime of a node is not over 98% alert you. The alerts can be notifications sent to Slack, Campfire, HipChat and PagerDuty. Tracing - Trace tracks how requests propagate through your application and receive detailed near real time performance insights. It auto analyses to generate latency reports to show you performance degradations on your VMs. What this means is that if your VM or app slows down tracing will pick this up. Logging - Logging allow you to store, search, analyst, monitor and alert on log data and events from GCP. Logging is fully-managed that can scale and ingest logs from thousands of VMs. You can also analyze this in real-time. IAMRoles: predefined primitive custom When you assign both predefined and primitive roles to a user, the permissions granted are a union of each role’s permissions Member types : Google account (Single person) Service account (Not a person) can be used by services and applications running on your Google Compute Engine instance to interact with other Google Cloud Platform APIs An instance can have only one service account. Two types of service accounts are available to Compute Engine instances: - User-managed service accounts - Google-managed service accounts Google Group (Multiple people) DiskStandard persistent disk vs SSD, is standard persistent disk HDD?preemptible instance will self close after running 24 hoursVm snapshots are used for backup operation or transfer of data, it’s a copy of the disk , snapshot already contains the os DataprepDataflowPub/subHadoop eco mapping to GCPHive -&gt; BigQueryHBase -&gt; BigTableHBase, Hive, Pig jobs -&gt; can run in DataprocSpark + Pig -&gt; DataflowKafka -&gt; Pub/subHDFS -&gt; GCS","categories":[{"name":"Cloud","slug":"Cloud","permalink":"http://www.philsblog.info/categories/Cloud/"},{"name":"GCP","slug":"Cloud/GCP","permalink":"http://www.philsblog.info/categories/Cloud/GCP/"},{"name":"Fundamentals","slug":"Cloud/GCP/Fundamentals","permalink":"http://www.philsblog.info/categories/Cloud/GCP/Fundamentals/"}],"tags":[{"name":"GCP","slug":"GCP","permalink":"http://www.philsblog.info/tags/GCP/"},{"name":"BigQuery","slug":"BigQuery","permalink":"http://www.philsblog.info/tags/BigQuery/"},{"name":"Dataproc","slug":"Dataproc","permalink":"http://www.philsblog.info/tags/Dataproc/"},{"name":"Cloud ML Engine","slug":"Cloud-ML-Engine","permalink":"http://www.philsblog.info/tags/Cloud-ML-Engine/"},{"name":"GCS","slug":"GCS","permalink":"http://www.philsblog.info/tags/GCS/"},{"name":"GCE","slug":"GCE","permalink":"http://www.philsblog.info/tags/GCE/"},{"name":"Cloud SQL","slug":"Cloud-SQL","permalink":"http://www.philsblog.info/tags/Cloud-SQL/"},{"name":"Datastore","slug":"Datastore","permalink":"http://www.philsblog.info/tags/Datastore/"},{"name":"BigTable","slug":"BigTable","permalink":"http://www.philsblog.info/tags/BigTable/"},{"name":"Datalab","slug":"Datalab","permalink":"http://www.philsblog.info/tags/Datalab/"},{"name":"TensorFlow","slug":"TensorFlow","permalink":"http://www.philsblog.info/tags/TensorFlow/"},{"name":"ML APIs","slug":"ML-APIs","permalink":"http://www.philsblog.info/tags/ML-APIs/"}]},{"title":"Gradle fundamentals","slug":"Gradle-fundamentals","date":"2018-05-20T11:12:00.000Z","updated":"2020-04-26T11:38:23.776Z","comments":true,"path":"2018/05/20/Gradle-fundamentals.html","link":"","permalink":"http://www.philsblog.info/2018/05/20/Gradle-fundamentals.html","excerpt":"","text":"Build12gradle cleangradle build Run application1234567# add application pluginapply plugin 'application'# add main classmainClassName = 'com.mycompany.Main'gradle run","categories":[{"name":"Dev","slug":"Dev","permalink":"http://www.philsblog.info/categories/Dev/"},{"name":"Gradle","slug":"Dev/Gradle","permalink":"http://www.philsblog.info/categories/Dev/Gradle/"}],"tags":[{"name":"Tools","slug":"Tools","permalink":"http://www.philsblog.info/tags/Tools/"},{"name":"Gradle","slug":"Gradle","permalink":"http://www.philsblog.info/tags/Gradle/"}]},{"title":"Agile","slug":"Agile","date":"2018-05-14T11:58:25.000Z","updated":"2020-04-26T11:36:41.207Z","comments":true,"path":"2018/05/14/Agile.html","link":"","permalink":"http://www.philsblog.info/2018/05/14/Agile.html","excerpt":"","text":"To what I understand about the agile methodology, it’s not about whether we stand up or not in the meeting, or what’s the name of the roles in the team. It’s about two things, the values/principles of agile and the procedures in the cycle. The values/principles make sure that everyone in the team are following the same rule. How does a team become agaile? They make decisions based on Agile values and principles. The decision make process is how a team becomes Agile. The values and principles have enough flexibility to allow teams in a wide variety of organisations to develop software in the ways that work best for their particular situation while providing enough direction to help a team continualy move toward their full potential. Manifesto (Values) Individuals and interactions over processes and tools个体和互动 高于 流程和工具 Working software over comprehensive documentation工作的软件 高于 详尽的文档 Customer collaboration over contract negotiation客户合作 高于 合同谈判 Responding to change over following a plan响应变化 高于 遵循计划 Principles Our highest priority is to satisfy the customer through early and continuous delivery of valuable software.我们最重要的目标，是通过持续不断地及早交付有价值的软件使客户满意。 Welcome changing requirements, even late in development. Agile processes harness change for the customer’s competitive advantage.欣然面对需求变化，即使在开发后期也一样。为了客户的竞争优势，敏捷过程掌控变化。 Deliver working software frequently, from a couple of weeks to a couple of months, with a preference to the shorter timescale.经常地交付可工作的软件，相隔几星期或一两个月，倾向于采取较短的周期。 Business people and developers must work together daily throughout the project.业务人员和开发人员必须相互合作，项目中的每一天都不例外。 Build projects around motivated individuals. Give them the environment and support they need, and trust them to get the job done.激发个体的斗志，以他们为核心搭建项目。提供所需的环境和支援，辅以信任，从而达成目标。 The most efficient and effective method of conveying information to and within a development team is face-to-face conversation.不论团队内外，传递信息效果最好效率也最高的方式是面对面的交谈。 Working software is the primary measure of progress.可工作的软件是进度的首要度量标准。 Agile processes promote sustainable development. The sponsors, developers, and users should be able to maintain a constant pace indefinitely.敏捷过程倡导可持续开发。责任人、开发人员和用户要能够共同维持其步调稳定延续。 Continuous attention to technical excellence and good design enhances agility.坚持不懈地追求技术卓越和良好设计，敏捷能力由此增强。 Simplicity–the art of maximizing the amount of work not done–is essential.以简洁为本，它是极力减少不必要工作量的艺术。 The best architectures, requirements, and designs emerge from self-organizing teams.最好的架构、需求和设计出自自组织团队。 At regular intervals, the team reflects on how to become more effective, then tunes and adjusts its behavior accordingly.团队定期地反思如何能提高成效，并依此调整自身的举止表现。 The procedures in scrum(One common impl of Agile) There are three main roles here: The one that comes up with the brightest ideas, defines the features in the product, known as product owner. The servant leader to the team, running all the meetings and keep things going, known as scrum master(Our PMs do that I guess). The team memembers, consist of developers, testers or anyone else that helps in building the product. Three artifacts: Product Backlog - the prioritized list of features that created by product owner, known as user stories, it could go in to the product. The list evolves and changes priority with every sprint. User stories - a way of describing a freature set that follows a specific writing format, this format allows the product owner to specify the right amount of deteail for the team to estimate the size of the task. The highest priority user stories go into the sprint backlog. They are estimated for size and committed for the next sprint. Burndown Chart - show the progress during a sprint on the completion of tasks in the sprint backlog. Three Ceremonies Sprint Planning - everyone in the project meet to discuss the user storeis and estimate their relatvie sizes. Daily stand up meeting - what’s done, what are you working on, and what help one needs Retrospective - at the end ot the sprint, the team demonstrates the completed work to the product owner. Discuss whay they can do to improve the process going forward. The benefits after using Agile: It raises visibility of what was going on. The prioritisation discussions were a lot more clear. Customer satisfaction by rapid, continuous delivery of useful software. Close, daily cooperation between business people and developers. Continuous attention to technical excellence and good design. Regular adaptation to changing circumstances. References:https://www.youtube.com/watch?v=9TycLR0TqFAhttps://www.youtube.com/watch?v=Z9QbYZh1YXYhttps://www.youtube.com/watch?v=NrHpXvDXVrwhttp://agilemanifesto.org/iso/zhchs/manifesto.html","categories":[{"name":"Dev","slug":"Dev","permalink":"http://www.philsblog.info/categories/Dev/"},{"name":"Theory","slug":"Dev/Theory","permalink":"http://www.philsblog.info/categories/Dev/Theory/"},{"name":"Methodology","slug":"Dev/Theory/Methodology","permalink":"http://www.philsblog.info/categories/Dev/Theory/Methodology/"}],"tags":[{"name":"Agile","slug":"Agile","permalink":"http://www.philsblog.info/tags/Agile/"},{"name":"scrum","slug":"scrum","permalink":"http://www.philsblog.info/tags/scrum/"},{"name":"kanban","slug":"kanban","permalink":"http://www.philsblog.info/tags/kanban/"}]},{"title":"Design Patterns in JDK","slug":"Design-Patterns-in-JDK","date":"2018-02-21T11:35:27.000Z","updated":"2020-04-26T11:36:44.246Z","comments":true,"path":"2018/02/21/Design-Patterns-in-JDK.html","link":"","permalink":"http://www.philsblog.info/2018/02/21/Design-Patterns-in-JDK.html","excerpt":"","text":"Design Pattern Example in JDK Description Singleton Runtime (hungry)NumberFormat Factory Integer.valueOfCLass.forName Replace constructorMethod name is more informative than constructor Factory Method Collection.oterator Abstract Factory java.sqlUIManager Builder DocumentBuilder Prototype Object.cloneCloneable Adapter java.io.InputStreamReader(InputStream)java.io.OutputStreamWriter(OutputStream) Bridge Handler &amp; Formatter in java.util.logging Composite javax.swing.JComponent#add Decorator java.io Facade Flyweight String constant pool Proxy RMI Iterator Iterator Observer java.util.Observer, ObservableListener Mediator Swing ButtonGroup Template method Strategy Chain of Responsibility ClassLoaderjava.util.logging.Logger Command RunnableCallableThreadPoolExecutor Interpreter java.util.regex.Pattern","categories":[{"name":"Dev","slug":"Dev","permalink":"http://www.philsblog.info/categories/Dev/"},{"name":"Theory","slug":"Dev/Theory","permalink":"http://www.philsblog.info/categories/Dev/Theory/"},{"name":"Design Pattern","slug":"Dev/Theory/Design-Pattern","permalink":"http://www.philsblog.info/categories/Dev/Theory/Design-Pattern/"}],"tags":[{"name":"Design Pattern","slug":"Design-Pattern","permalink":"http://www.philsblog.info/tags/Design-Pattern/"}]},{"title":"Rest VS GraphQL","slug":"Rest-VS-GraphQL","date":"2017-10-29T02:45:32.000Z","updated":"2020-04-26T08:11:35.108Z","comments":true,"path":"2017/10/29/Rest-VS-GraphQL.html","link":"","permalink":"http://www.philsblog.info/2017/10/29/Rest-VS-GraphQL.html","excerpt":"","text":"Hi guys, I have read some articles about Rest vs GraphQL, I would like to summarize a bit of what I learn so far, and give my suggestions: Background:People used to have their model, view and controller all live on the server, getting data from models to view is ok because almost all the data is right there on the server. In modern JS or Mobile apps, that is no longer the case, the controllers and views now live mostly on the client, however, most of the data is still on the server. With all but the most custom RESTful APIs, fetching data from the server is both costly and complicated: the latency is high, and chances are you will either fetch more data than you need or make more roundtrips you would like — or both! That’s where GraphQL comes to the rescue. With REST, the server determines what data will be sent down to the clientproblem: app evolves, become more complex make separate endpoints for different views of the same object make many calls for to complete one result or create a new endpoint for this specific result. generic endpoints -&gt; make app slow, too many useless info within API calls, not specific enough. dynamic table (if let clients choose the data field) -&gt; cannot eliminating any old code (needs to remain compatibility with the old client), then we always create a new endpoint, leave the old ones untouched. With GraphQL, the server exposes a comprehensive data schema to the client, and the client decides exactly what it needs. Unlike with discrete REST endpoints, all the data for any given UI (page) can be sent in one trip to the client. GraphQL fits between your backend and your frontend. It lets you define a model for your data and a mapping between that model and your backends. It is not a replacement for your existing backend. GraphQL fetches your data in one call to the server instead of many, and they cache the data for you. GraphQL has an increasingly active ecosystem, with implementations in Java, Python, Ruby, Scala, .NET, Go, etc. Used with many different databases, can combine data from multiple sources into one GraphQL schema SQL DB MongoDB Even REST endpoint Here is tutorial of how to GraphQL Server Summary:Most of the articles are introducing this new tech in a positive attitude, and to myself, I do believe GraphQL is a better alternative to REST. However, I also suggest that we should take a deep look into our requirements, because I am not entirely familiar with FE (Web, mobile), so I am not sure what is the situation. To my understanding so far, GraphQL benefits the projects that: have more than one client (e.g. web + iOS) have a mobile client and care about latency and bandwidth are moving to a microservices architecture have a REST API that has gotten so complicated and it’s a significant drag on product development. want to decouple frontends and backends to speed up development If that is the case, then I highly recommend adopting this new tech. Even if it is not, I still think GraphQL is an excellent option to adopt when we got time, because it brings: Clean API between backends and frontends Less communication overhead and fewer meetings No more time spent writing API documentation No more time spent trying to figure out an API Great tooling for your API - GraphiQL, not GraphQL read it carefully.Here is a story of adopting GraphQL and you will find the benefits list above in it. By the way, here is another option call Falcor, designed by Netflix, which is similar to GraphQL, but with different syntax. Falcor is easier to learn but not as powerful as GraphQL. Also, Falcor is JS only; GraphQL has so many implementations. References:REST API downfalls, and dawn of GraphQLhttps://medium.com/@ottovw/rest-api-downfalls-and-dawn-of-graphql-dd00991a0eb8#.5kf08z2n7 Tutorial: How to build a GraphQL serverhttps://dev-blog.apollodata.com/tutorial-building-a-graphql-server-cddaa023c035#.pia6zcwlg GraphQL vs. Falcorhttps://dev-blog.apollodata.com/graphql-vs-falcor-4f1e9cbf7504#.xeearf6rs Just Because Github Has a GraphQL API Doesn’t Mean You Should Toohttps://www.programmableweb.com/news/just-because-github-has-graphql-api-doesn%E2%80%99t-mean-you-should-too/analysis/2016/09/21 GraphQL in the age of REST APIshttps://medium.com/chute-engineering/graphql-in-the-age-of-rest-apis-b10f2bf09bba#.ezbukzu35 Adopting GraphQLhttps://voice.kadira.io/adopting-graphql-2f00dfe0fda2#.htpdce1sm","categories":[],"tags":[]},{"title":"IDEA Hotkeys","slug":"Eclipse-and-IntelliJ-IDEA-Hotkeys","date":"2017-09-15T23:30:12.000Z","updated":"2020-04-26T11:40:00.466Z","comments":true,"path":"2017/09/16/Eclipse-and-IntelliJ-IDEA-Hotkeys.html","link":"","permalink":"http://www.philsblog.info/2017/09/16/Eclipse-and-IntelliJ-IDEA-Hotkeys.html","excerpt":"","text":"Description Eclipse IntelliJ IDEA VS Code show the type hierachy F4 ctrl+h Findusages Ctrl+Shift+G ⌥F7 Go to line Ctrl+L ⌘L Open resource Ctrl+Shift+R ⇧⌘O Open type Ctrl+Shift+T ⌘O Back Ctrl+[ ⌘[ ⇧⌘[ Forward Ctrl+] ⌘] ⇧⌘] Rename Ctrl+Alt+R ⇧F6 Go to Impl Ctrl+mouse click ⌥+⌘+b","categories":[{"name":"Ops","slug":"Ops","permalink":"http://www.philsblog.info/categories/Ops/"},{"name":"Hotkeys","slug":"Ops/Hotkeys","permalink":"http://www.philsblog.info/categories/Ops/Hotkeys/"}],"tags":[{"name":"Eclipse","slug":"Eclipse","permalink":"http://www.philsblog.info/tags/Eclipse/"},{"name":"IntelliJ","slug":"IntelliJ","permalink":"http://www.philsblog.info/tags/IntelliJ/"}]},{"title":"Dockerfile reference","slug":"Dockerfile-reference","date":"2017-09-15T23:26:28.000Z","updated":"2020-04-26T11:39:42.217Z","comments":true,"path":"2017/09/16/Dockerfile-reference.html","link":"","permalink":"http://www.philsblog.info/2017/09/16/Dockerfile-reference.html","excerpt":"","text":"Use Dockerfike create you own docker image. DockerFile contains basic image info mainainer info image CMD container CMD 1234567891011121314#第一行必须指令基于的基础镜像From ubutu#维护者信息MAINTAINER docker_user docker_user@mail.com#镜像的操作指令apt/sourcelist.listRUN apt-get update &amp;&amp; apt-get install -y ngnix RUN echo &quot;\\ndaemon off;&quot;&gt;&gt;/etc/ngnix/nignix.conf#容器启动时执行指令CMD /usr/sbin/ngnix 1、From指令From 或者From : DockerFile第一条必须为From指令。如果同一个DockerFile创建多个镜像时，可使用多个From指令（每个镜像一次） 2、MAINTAINER格式为maintainer ，指定维护者的信息 3、RUN格式为Run 或者Run [“executable” ,”Param1”, “param2”]前者在shell终端上运行，即/bin/sh -C，后者使用exec运行。例如：RUN [“/bin/bash”, “-c”,”echo hello”]每条run指令在当前基础镜像执行，并且提交新镜像。当命令比较长时，可以使用“/”换行。 4、CMD指令支持三种格式：CMD [“executable” ,”Param1”, “param2”]使用exec执行，推荐CMD command param1 param2，在/bin/sh上执行CMD [“Param1”, “param2”] 提供给ENTRYPOINT做默认参数。 每个容器只能执行一条CMD命令，多个CMD命令时，只最后一条被执行。 5、EXPOSE 格式为 EXPOSE […] 。 告诉Docker服务端容器暴露的端口号，供互联系统使用。在启动Docker时，可以通过-P,主机会自动分配一个端口号转发到指定的端口。使用-P，则可以具体指定哪个本地端口映射过来 例如：EXPOSE 22 80 8443 6、ENV 格式为 ENV 。 指定一个环境变量，会被后续 RUN 指令使用，并在容器运行时保持。 1234ENV PG_MAJOR 9.3ENV PG_VERSION 9.3.4RUN curl -SL http://example.com/postgres-$PG_VERSION.tar.xz | tar -xJC /usr/src/postgress &amp;&amp; …ENV PATH /usr/local/postgres-$PG_MAJOR/bin:$PATH 7、ADD格式为 ADD 。 该命令将复制指定的 到容器中的 。 其中 可以是Dockerfile所在目录的一个相对路径；也可以是一个URL；还可以是一个tar文件（自动解压为目录）。则。 8、COPY 格式为 COPY 。 复制本地主机的 （为Dockerfile所在目录的相对路径）到容器中的 。 当使用本地目录为源目录时，推荐使用 COPY 。 9、ENTRYPOINT 两种格式： ENTRYPOINT [“executable”, “param1”, “param2”]ENTRYPOINT command param1 param2 （shell中执行）。配置容器启动后执行的命令，并且不可被 docker run 提供的参数覆盖。 每个Dockerfile中只能有一个 ENTRYPOINT ，当指定多个时，只有最后一个起效。 10、VOLUME 格式为 VOLUME [“/data”] 。 创建一个可以从本地主机或其他容器挂载的挂载点，一般用来存放数据库和需要保持的数据等。 11、USER 格式为 USER daemon 。 指定运行容器时的用户名或UID，后续的 RUN 也会使用指定用户。 当服务不需要管理员权限时，可以通过该命令指定运行用户。并且可以在之前创建所需要的用户，例如： RUN groupadd -r postgres &amp;&amp; useradd -r -g postgres postgres 。要临时获取管理员权限可以使用 gosu ，而不推荐 sudo 。 12、WORKDIR 格式为 WORKDIR /path/to/workdir 。 为后续的 RUN 、 CMD 、 ENTRYPOINT 指令配置工作目录。 可以使用多个 WORKDIR 指令，后续命令如果参数是相对路径，则会基于之前命令指定的路径。例如 WORKDIR /aWORKDIR bWORKDIR cRUN pwd则最终路径为 /a/b/c 。 13、ONBUILD 格式为 ONBUILD [INSTRUCTION] 。 配置当所创建的镜像作为其它新创建镜像的基础镜像时，所执行的操作指令。 例如，Dockerfile使用如下的内容创建了镜像 image-A 。 […]ONBUILD ADD . /app/srcONBUILD RUN /usr/local/bin/python-build –dir /app/src[…]如果基于A创建新的镜像时，新的Dockerfile中使用 FROM image-A 指定基础镜像时，会自动执行 ONBUILD 指令内容，等价于在后面添加了两条指令。123456FROM image-A#Automatically run the followingADD . /app/srcRUN /usr/local/bin/python-build --dir /app/src使用 ONBUILD 指令的镜像，推荐在标签中注明，例如 ruby:1.9-onbuild 。 三、创建镜像通过Docker Build 创建镜像。命令读取指定路径下（包括子目录）所有的Dockefile，并且把目录下所有内容发送到服务端，由服务端创建镜像。另外可以通过创建.dockerignore文件（每一行添加一个匹配模式）让docker忽略指定目录或者文件 格式为Docker Build [选项] 路径需要制定标签信息，可以使用-t选项例如：Dockerfile路径为 /tmp/docker_build/，生成镜像的标签为build_repo/my_images$dudo docker build -t build_repo/my_images /tmp/docker_build/","categories":[{"name":"Ops","slug":"Ops","permalink":"http://www.philsblog.info/categories/Ops/"},{"name":"Docker","slug":"Ops/Docker","permalink":"http://www.philsblog.info/categories/Ops/Docker/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"http://www.philsblog.info/tags/Docker/"}]},{"title":"Spring Bean","slug":"Spring-Bean","date":"2017-09-04T01:15:45.000Z","updated":"2020-04-26T11:30:43.985Z","comments":true,"path":"2017/09/04/Spring-Bean.html","link":"","permalink":"http://www.philsblog.info/2017/09/04/Spring-Bean.html","excerpt":"","text":"Bean attributes Attribute Description class Necessary, define the class used to create the bean name unique name identifier, you can use “id” or “name” in XML scope Define the scope of bean constructor-arg DI properties DI autowiring mode DI lazy-initialization mode Delay the init until the first use initialization Define the method that initialize bean destruction Define the method that destruction bean Spring config meta dataThere are three ways of Spring container configuration Base on XML file Base on annotation Base on Java An example of Spring XML config file123456789101112131415161718192021222324252627282930&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-3.0.xsd\"&gt; &lt;!-- A simple bean definition --&gt; &lt;bean id=\"...\" class=\"...\"&gt; &lt;!-- collaborators and configuration for this bean go here --&gt; &lt;/bean&gt; &lt;!-- A bean definition with lazy init set on --&gt; &lt;bean id=\"...\" class=\"...\" lazy-init=\"true\"&gt; &lt;!-- collaborators and configuration for this bean go here --&gt; &lt;/bean&gt; &lt;!-- A bean definition with initialization method --&gt; &lt;bean id=\"...\" class=\"...\" init-method=\"...\"&gt; &lt;!-- collaborators and configuration for this bean go here --&gt; &lt;/bean&gt; &lt;!-- A bean definition with destruction method --&gt; &lt;bean id=\"...\" class=\"...\" destroy-method=\"...\"&gt; &lt;!-- collaborators and configuration for this bean go here --&gt; &lt;/bean&gt; &lt;!-- more bean definitions go here --&gt;&lt;/beans&gt; Bean scopeThe scope of bean must be defined. The default scope is singleton Scope Description singleton This scopes the bean definition to a single instance per Spring IoC container (default). prototype This scopes a single bean definition to have any number of object instances. request This scopes a bean definition to an HTTP request. Only valid in the context of a web-aware Spring ApplicationContext. session This scopes a bean definition to an HTTP session. Only valid in the context of a web-aware Spring ApplicationContext. global-session This scopes a bean definition to a global HTTP session. Only valid in the context of a web-aware Spring ApplicationContext. Bean lifecycle","categories":[{"name":"Dev","slug":"Dev","permalink":"http://www.philsblog.info/categories/Dev/"},{"name":"Spring","slug":"Dev/Spring","permalink":"http://www.philsblog.info/categories/Dev/Spring/"}],"tags":[{"name":"Spring","slug":"Spring","permalink":"http://www.philsblog.info/tags/Spring/"}]},{"title":"Spring Framework Intro","slug":"Spring-Framework","date":"2017-09-01T23:22:08.000Z","updated":"2020-04-26T11:30:46.423Z","comments":true,"path":"2017/09/02/Spring-Framework.html","link":"","permalink":"http://www.philsblog.info/2017/09/02/Spring-Framework.html","excerpt":"","text":"BeanFactoryIt’s a simple container which provides Dependency Injection. It’s defined in org.springframework.beans.factory.BeanFactor.There are many instances of BeanFactory, one of the most used is XmlBeanFactory, it reads from a XML file to config the meta data. ApplicationContextIt’s similar to BeanFactory, it loads the defination of the beans from the config file. It’s defined in org.springframework.context.ApplicationContext interface. ApplicationContext contains all BeanFactor’s functions. Normally, ApplicationContext is recommended. BeanFactory will be used in light-weight apps. FileSystemXmlApplicationContextIt reads from XML file, and the path of the file is needed ClassPathXmlApplicationContextThe path of xml file should be included in the CLASSPATH WebXmlApplicationContext 123456789101112131415# ClassPathXmlApplicationContext public static void main(String[] args) &#123; ApplicationContext context = new ClassPathXmlApplicationContext(\"Beans.xml\"); HelloWorld obj = (HelloWorld) context.getBean(\"helloWorld\"); obj.getMessage(); &#125;# FileSystemXmlApplicationContext public static void main(String[] args) &#123; ApplicationContext context = new FileSystemXmlApplicationContext (\"C:/Users/ZARA/workspace/HelloSpring/src/Beans.xml\"); HelloWorld obj = (HelloWorld) context.getBean(\"helloWorld\"); obj.getMessage(); &#125;","categories":[{"name":"Dev","slug":"Dev","permalink":"http://www.philsblog.info/categories/Dev/"},{"name":"Spring","slug":"Dev/Spring","permalink":"http://www.philsblog.info/categories/Dev/Spring/"}],"tags":[{"name":"Spring","slug":"Spring","permalink":"http://www.philsblog.info/tags/Spring/"}]},{"title":"Backup hexo and auto deploy with Jenkins","slug":"Backup-hexo-and-auto-deploy-with-Jenkins","date":"2017-08-21T11:30:38.000Z","updated":"2020-04-26T11:39:37.899Z","comments":true,"path":"2017/08/21/Backup-hexo-and-auto-deploy-with-Jenkins.html","link":"","permalink":"http://www.philsblog.info/2017/08/21/Backup-hexo-and-auto-deploy-with-Jenkins.html","excerpt":"","text":"Finaly, I am able to update the blog again. I was trying to back up the whole project of the hexo blog and messed up……I guess it’s easier for you to watch other people’s experience rather than do it your self. Hexo only deploy the generated html files, however if you want start over after you computer crashed, or you wanted to write something on a different machine, it will be impossible unless you copy all of your existing work and write them again. So I started to backup everything. Here is how I did, just in case I forget. 1 Clone the deplyed project to a new folder2 Create and checkout to a new branch3 Git rm everything4 Copy config.yml, themes/, source, scffolds, package.json, .gitignore to this new folder5 Remove the .git and .gitignore from your themes folder6 Git add all of the copied files and folders, commit and push At this stage, you will be able to1) backup the project by git add commit and push in the new branch,2) and deploy hexo by hexo clean generate deploy to the old branch. But it takes two steps! We lazy people move the world forward, right? So I’d like to use jenkins deploy it for me! So you need a Linux environment with git npm hexo-cli hexo-server installed and ssh key setup for github, then: 1 Download the war version of jenkins2 run it java -jar jenkins.war &amp;&gt; somelogs &amp;3 Install all default plugins and also install github intigration(we use this one for webhook, it was called GitHub plugin, however it chaged to github intigration when I install it, who knows what it will be called when you install) 4 Setup the build on jenkins, add your repo, choose the new branch we just created.5 Add trigger 6 Add Webhook on you github repo(go to that repo’s settings) 7 Add the following cmd to your build123456npm -vrm -rf node_modules/npm installhexo cleanhexo generatehexo deploy You should be good by now, when you do a push something to the new branch, jenkins will pull you changes, build, and deploy it to github pages for you! Don’t forget add ssh key to ssh-agent and make sure the npm version is correct. Have fun, happing blogging","categories":[{"name":"Ops","slug":"Ops","permalink":"http://www.philsblog.info/categories/Ops/"},{"name":"Jenkins","slug":"Ops/Jenkins","permalink":"http://www.philsblog.info/categories/Ops/Jenkins/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"http://www.philsblog.info/tags/Hexo/"},{"name":"Git","slug":"Git","permalink":"http://www.philsblog.info/tags/Git/"}]},{"title":"Big O Cheat sheets","slug":"Big-O-Cheat-sheets","date":"2017-08-15T14:15:43.000Z","updated":"2020-04-26T11:36:38.139Z","comments":true,"path":"2017/08/16/Big-O-Cheat-sheets.html","link":"","permalink":"http://www.philsblog.info/2017/08/16/Big-O-Cheat-sheets.html","excerpt":"","text":"Algorithm Structure Time:Best Time:Average Time:Worst Space:Worst Quick sort Array O(n log(n)) O(n log(n)) O(n2) O(n) Merge sort Array O(n log(n)) O(n log(n)) O(n log(n)) O(n) Heap sort Array O(n) O(n log(n)) O(n log(n)) O(1) Smooth sort Array O(n) O(n log(n)) O(n log(n)) O(1) Bubble sort Array O(n) O(n2) O(n2) O(1) Insertion sort Array O(n) O(n2) O(n2) O(1) Selection sort Array O(n2) O(n2) O(n2) O(1) Check this out","categories":[{"name":"Dev","slug":"Dev","permalink":"http://www.philsblog.info/categories/Dev/"},{"name":"Theory","slug":"Dev/Theory","permalink":"http://www.philsblog.info/categories/Dev/Theory/"},{"name":"Algorithm","slug":"Dev/Theory/Algorithm","permalink":"http://www.philsblog.info/categories/Dev/Theory/Algorithm/"}],"tags":[{"name":"Sort","slug":"Sort","permalink":"http://www.philsblog.info/tags/Sort/"},{"name":"Algorithm","slug":"Algorithm","permalink":"http://www.philsblog.info/tags/Algorithm/"},{"name":"big O","slug":"big-O","permalink":"http://www.philsblog.info/tags/big-O/"}]},{"title":"Linux Configuration Files","slug":"Linux-Configuration-Files","date":"2017-07-15T03:53:56.000Z","updated":"2020-04-26T08:11:35.107Z","comments":true,"path":"2017/07/15/Linux-Configuration-Files.html","link":"","permalink":"http://www.philsblog.info/2017/07/15/Linux-Configuration-Files.html","excerpt":"","text":"mysql configuration file~/.my.cnf 启动引导程序配置文件12LILO /etc/lilo.confGRUB /boot/grub/menu.lst 系统启动文件核脚本1234主启动控制文件 /etc/inittabSysV启动脚本的位置 /etc/init.d、/etc/rc.d/init.d或/etc/rc.dSysV启动脚本链接的位置 /etc/init.d/rc.d、/etc/rc.d/rc.d或/etc/rc.d本地启动脚本 /etc/rc.d/rc.local、/etc/init.d/boot.local或/etc/rc.boot里的文件 网络配置文件1234建立网络接口的脚本 /sbin/ifup保存网络配置数据文件的目录 /etc/network、/etc/sysconfig/network和/etc/sysconfig/network-scripts保存解析DNS服务的文件 /etc/resolv.confDHCP客户端的配置文件 /etc/dhclient.conf 超级服务程序配置文件和目录123inetd配置文件 /etc/inetd.confTCP Wrappers配置文件 /etc/hosts.allow和/etc/hosts.denyxinetd配置文件 /etc/xinetd.conf和/etc/xinetd.d目录里的文件 硬件配置1内核模块配置文件 /etc/modules.conf 硬件访问文件12Linux设备文件 /dev目录里保存硬件和驱动程序数据的文件 /proc目录里 扫描仪配置文件12SANE主配置 /etc/sane.d/dll.conf特定扫描仪的配置文件 /etc/sane.d目录里以扫描仪型号命名的文件 打印机配置文件1234BSD LPD核LPRng的本地打印机主配置文件 /etc/printcapCUPS本地打印机主配置和远程访问受权文件 /etc/cups/cupsd.confBSD LPD远程访问受权文件 /etc/hosts.lpdLPRng远程访问受权文件 /etc/lpd.perms 文件系统123文件系统表 /etc/fstab软驱装配点 /floppy 、/mnt/floppy 或/media/floppy光驱装配点 /cdrom 、/mnt/cdrom 或/media/cdrom shell 配置文件1234bash 系统非登录配置文件 /etc/bashrc 、/etc/bash.bashrc 或/etc/bash.bashrc.localbash 系统登录文件 /etc/profile 和/etc/profile.d 里的文件bash 用户非登录配置文件 ~/.bashrcbash 用户登录配置文件 ~/.profile XFree86 配置文件核目录12345XFree86 主配置文件 /etc/XF86config 、/etc/X11/XF86Config 或/etc/X11/XF86Config-4字体服务程序配置文件 /etc/X11/fs/configXft 1.x 配置文件 /etcX11/XftConfigXft 2.0 配置文件 /etc/fonts/fonts.conf字体目录 /usr/X11R6/lib/X11/fonts 和/usr/share/fonts Web 服务程序配置文件12Apache 主配置文件 /etc/apache 、/etc/httpd 或/httpd/conf 里的httpd.conf 或httpd2.conf 文件MIME 类型文件 与Apache 主配置文件在同一目录里的mime.types 或apache-mime.types 文件服务程序配置文件123456ProFTPd 配置文件 /etc/proftpd.confvsftpd 配置文件 /etc/vsftpd.confNFS 服务程序的输出定义文件 /etc/exportsNFS 客户端装配的NFS 输出 /etc/fstabSamba 配置文件 /etc/samba/smb.confSamba 用户配置文件 /etc/samba/smbpasswd 邮件服务程序配置文件123456sendmail 主配置文件 /etc/mail/sendmail.cfsendmail 源配置文件 /etc/mail/sendmail.mc 或/usr/share/sendmail/cf/cf/linux.smtp.mc 或其他文件Postfix 主配置文件 /etc/postfix/main.cfExim 主配置文件 /etc/exim/exim.cfProcmail 配置文件 /etc/procmailrc 或~/.procmailrcFetchmail 配置文件 ~/.fetchmailrc 远程登录配置文件12345SSH 服务程序配置文件 /etc/ssh/sshd_configSSH 客户端配置文件 /etc/ssh/ssh_configXDM 配置文件 /etc/X11/xdm 目录下GDM 配置文件 /etc/X11/gdm 目录下VNC 服务程序配置文件 /usr/X11R6/bin/vncserver 启动脚本和~/.vnc 目录里的文件 其他服务程序配置文件123DHCP 服务程序配置文件 /etc/dhcpd.confBIND 服务程序配置文件 /etc/named.conf 和/var/named/NTP 服务程序配置文件 /etc/ntp.conf Others1234567891011121314# Lists commands and times to run them for the cron deamon./etc/crontab # Specifies how host names are resolved./etc/host.conf # List hosts for name lookup use that are locally required./etc/hosts # A list of currently mounted file systems. Setup by boot scripts and updated by the mount command. /etc/mtab # Scripts or directories of scripts to run at startup or when changing run level. /etc/rc or /etc/rc.d or /etc/rc?.d # User aliases, path modifier, and functions.$HOME/.bashrc # Users environment stuff and startup programs.$HOME/.bash_profile","categories":[],"tags":[]},{"title":"JVM parameters","slug":"JVM-parameters","date":"2017-06-15T13:35:19.000Z","updated":"2020-04-26T11:29:53.958Z","comments":true,"path":"2017/06/15/JVM-parameters.html","link":"","permalink":"http://www.philsblog.info/2017/06/15/JVM-parameters.html","excerpt":"","text":"参数名称 含义 默认值 -Xms 初始堆大小 物理内存的1/64(&lt;1GB) 默认(MinHeapFreeRatio参数可以调整)空余堆内存小于40%时，JVM就会增大堆直到-Xmx的最大限制. -Xmx 最大堆大小 物理内存的1/4(&lt;1GB) 默认(MaxHeapFreeRatio参数可以调整)空余堆内存大于70%时，JVM会减少堆直到 -Xms的最小限制 -Xmn 年轻代大小(1.4or lator) 注意：此处的大小是（eden+ 2 survivor space).与jmap -heap中显示的New gen是不同的。整个堆大小=年轻代大小 + 年老代大小 + 持久代大小. 增大年轻代后,将会减小年老代大小.此值对系统性能影响较大,Sun官方推荐配置为整个堆的3/8 -XX:NewSize 设置年轻代大小(for 1.3/1.4) -XX:MaxNewSize 年轻代最大值(for 1.3/1.4) -XX:PermSize 设置持久代(perm gen)初始值 物理内存的1/64 -XX:MaxPermSize 设置持久代最大值 物理内存的1/4 -Xss 每个线程的堆栈大小 JDK5.0以后每个线程堆栈大小为1M,以前每个线程堆栈大小为256K.更具应用的线程所需内存大小进行 调整.在相同物理内存下,减小这个值能生成更多的线程.但是操作系统对一个进程内的线程数还是有限制的,不能无限生成,经验值在3000~5000左右 一般小的应用， 如果栈不是很深， 应该是128k够用的 大的应用建议使用256k。这个选项对性能影响比较大，需要严格的测试。（校长） 和threadstacksize选项解释很类似,官方文档似乎没有解释,在论坛中有这样一句话:”-Xss is translated in a VM flag named ThreadStackSize” 一般设置这个值就可以了。","categories":[{"name":"Dev","slug":"Dev","permalink":"http://www.philsblog.info/categories/Dev/"},{"name":"Java","slug":"Dev/Java","permalink":"http://www.philsblog.info/categories/Dev/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://www.philsblog.info/tags/Java/"},{"name":"jvm","slug":"jvm","permalink":"http://www.philsblog.info/tags/jvm/"}]},{"title":"An A-Z Index of the Bash command line for Linux","slug":"An-A-Z-Index-of-the-Bash-command-line-for-Linux","date":"2017-06-15T12:16:10.000Z","updated":"2020-04-26T11:39:33.652Z","comments":true,"path":"2017/06/15/An-A-Z-Index-of-the-Bash-command-line-for-Linux.html","link":"","permalink":"http://www.philsblog.info/2017/06/15/An-A-Z-Index-of-the-Bash-command-line-for-Linux.html","excerpt":"","text":"name Description alias Create an alias • apropos Search Help manual pages (man -k) apt-get Search for and install software packages (Debian/Ubuntu) aptitude Search for and install software packages (Debian/Ubuntu) aspell Spell Checker awk Find and Replace text, database sort/validate/index basename Strip directory and suffix from filenames bash GNU Bourne-Again SHell bc Arbitrary precision calculator language bg Send to background bind Set or display readline key and function bindings • break Exit from a loop • builtin Run a shell builtin bzip2 Compress or decompress named file(s) cal Display a calendar case Conditionally perform a command cat Concatenate and print (display) the content of files cd Change Directory cfdisk Partition table manipulator for Linux chattr Change file attributes on a Linux file system chgrp Change group ownership chmod Change access permissions chown Change file owner and group chroot Run a command with a different root directory chkconfig System services (runlevel) cksum Print CRC checksum and byte counts clear Clear terminal screen cmp Compare two files comm Compare two sorted files line by line command Run a command - ignoring shell functions • continue Resume the next iteration of a loop • cp Copy one or more files to another location cron Daemon to execute scheduled commands crontab Schedule a command to run at a later time csplit Split a file into context-determined pieces curl Transfer data from or to a server cut Divide a file into several parts date Display or change the date &amp; time dc Desk Calculator dd Convert and copy a file, write disk headers, boot records ddrescue Data recovery tool declare Declare variables and give them attributes • df Display free disk space diff Display the differences between two files diff3 Show differences among three files dig DNS lookup dir Briefly list directory contents dircolors Colour setup for ‘ls’ dirname Convert a full pathname to just a path dirs Display list of remembered directories dmesg Print kernel &amp; driver messages du Estimate file space usage echo Display message on screen • egrep Search file(s) for lines that match an extended expression eject Eject removable media enable Enable and disable builtin shell commands • env Environment variables ethtool Ethernet card settings eval Evaluate several commands/arguments exec Execute a command exit Exit the shell expect Automate arbitrary applications accessed over a terminal expand Convert tabs to spaces export Set an environment variable expr Evaluate expressions false Do nothing, unsuccessfully fdformat Low-level format a floppy disk fdisk Partition table manipulator for Linux fg Send job to foreground fgrep Search file(s) for lines that match a fixed string file Determine file type find Search for files that meet a desired criteria fmt Reformat paragraph text fold Wrap text to fit a specified width. for Expand words, and execute commands format Format disks or tapes free Display memory usage fsck File system consistency check and repair ftp File Transfer Protocol function Define Function Macros fuser Identify/kill the process that is accessing a file gawk Find and Replace text within file(s) getopts Parse positional parameters grep Search file(s) for lines that match a given pattern groupadd Add a user security group groupdel Delete a group groupmod Modify a group groups Print group names a user is in gzip Compress or decompress named file(s) hash Remember the full pathname of a name argument head Output the first part of file(s) help Display help for a built-in command • history Command History hostname Print or set system name htop Interactive process viewer iconv Convert the character set of a file id Print user and group id’s if Conditionally perform a command ifconfig Configure a network interface ifdown Stop a network interface ifup Start a network interface up import Capture an X server screen and save the image to file install Copy files and set attributes iostat Report CPU and i/o statistics ip Routing, devices and tunnels jobs List active jobs • join Join lines on a common field kill Kill a process by specifying its PID killall Kill processes by name less Display output one screen at a time let Perform arithmetic on shell variables • link Create a link to a file ln Create a symbolic link to a file local Create a function variable • locate Find files logname Print current login name logout Exit a login shell • look Display lines beginning with a given string lpc Line printer control program lpr Off line print lprint Print a file lprintd Abort a print job lprintq List the print queue lprm Remove jobs from the print queue lsattr List file attributes on a Linux second extended file system lsblk List block devices ls List information about file(s) lsof List open files lspci List all PCI devices make Recompile a group of programs man Help manual mkdir Create new folder(s) mkfifo Make FIFOs (named pipes) mkfile Make a file mkisofs Create an hybrid ISO9660/JOLIET/HFS filesystem mknod Make block or character special files mktemp Make a temporary file more Display output one screen at a time most Browse or page through a text file mount Mount a file system mtools Manipulate MS-DOS files mtr Network diagnostics (traceroute/ping) mv Move or rename files or directories mmv Mass Move and rename (files) nc Netcat, read and write data across networks netstat Networking connections/stats nice Set the priority of a command or job nl Number lines and write files nohup Run a command immune to hangups notify-send Send desktop notifications nslookup Query Internet name servers interactively open Open a file in its default application op Operator access passwd Modify a user password paste Merge lines of files pathchk Check file name portability ping Test a network connection pgrep List processes by name pkill Kill processes by name popd Restore the previous value of the current directory pr Prepare files for printing printcap Printer capability database printenv Print environment variables printf Format and print data • ps Process status pushd Save and then change the current directory pv Monitor the progress of data through a pipe pwd Print Working Directory quota Display disk usage and limits quotacheck Scan a file system for disk usage ram ram disk device rar Archive files with compression rcp Copy files between two machines read Read a line from standard input • readarray Read from stdin into an array variable • readonly Mark variables/functions as readonly reboot Reboot the system rename Rename files renice Alter priority of running processes remsync Synchronize remote files via email return Exit a shell function rev Reverse lines of a file rm Remove files rmdir Remove folder(s) rsync Remote file copy (Synchronize file trees) screen Multiplex terminal, run remote shells via ssh scp Secure copy (remote file copy) sdiff Merge two files interactively sed Stream Editor select Accept keyboard input seq Print numeric sequences set Manipulate shell variables and functions sftp Secure File Transfer Program shift Shift positional parameters shopt Shell Options shutdown Shutdown or restart linux sleep Delay for a specified time slocate Find files sort Sort text files source Run commands from a file ‘.’ split Split a file into fixed-size pieces ss Socket Statistics ssh Secure Shell client (remote login program) stat Display file or file system status strace Trace system calls and signals su Substitute user identity sudo Execute a command as another user sum Print a checksum for a file suspend Suspend execution of this shell • sync Synchronize data on disk with memory tail Output the last part of file tar Store, list or extract files in an archive tee Redirect output to multiple files test Evaluate a conditional expression time Measure Program running time timeout Run a command with a time limit times User and system times touch Change file timestamps top List processes running on the system tput Set terminal-dependent capabilities, color, position traceroute Trace Route to Host trap Execute a command when the shell receives a signal • tr Translate, squeeze, and/or delete characters true Do nothing, successfully tsort Topological sort tty Print filename of terminal on stdin type Describe a command • ulimit Limit user resources • umask Users file creation mask umount Unmount a device unalias Remove an alias • uname Print system information unexpand Convert spaces to tabs uniq Uniquify files units Convert units from one scale to another unrar Extract files from a rar archive unset Remove variable or function names unshar Unpack shell archive scripts until Execute commands (until error) uptime Show uptime useradd Create new user account userdel Delete a user account usermod Modify user account users List users currently logged in uuencode Encode a binary file uudecode Decode a file created by uuencode v Verbosely list directory contents (`ls -l -b’) vdir Verbosely list directory contents (`ls -l -b’) vi Text Editor vmstat Report virtual memory statistics w Show who is logged on and what they are doing wait Wait for a process to complete • watch Execute/display a program periodically wc Print byte, word, and line counts whereis Search the user’s $path, man pages and source files for a program which Search the user’s $path for a program file while Execute commands who Print all usernames currently logged in whoami Print the current user id and name (`id -un’) wget Retrieve web pages or files via HTTP, HTTPS or FTP write Send a message to another user xargs Execute utility, passing constructed argument list(s) xdg-open Open a file or URL in the user’s preferred application. xz Compress or decompress .xz and .lzma files yes Print a string until interrupted zip Package and compress (archive) files. . Run a command script in the current shell !! Run the last command again ### Comment / Remark A copy from https://ss64.com/bash/","categories":[{"name":"Ops","slug":"Ops","permalink":"http://www.philsblog.info/categories/Ops/"},{"name":"Linux","slug":"Ops/Linux","permalink":"http://www.philsblog.info/categories/Ops/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://www.philsblog.info/tags/Linux/"},{"name":"CLI","slug":"CLI","permalink":"http://www.philsblog.info/tags/CLI/"}]},{"title":"Most used Linux commands","slug":"Most-used-Linux-commands","date":"2017-05-13T02:22:32.000Z","updated":"2020-04-26T11:40:27.863Z","comments":true,"path":"2017/05/13/Most-used-Linux-commands.html","link":"","permalink":"http://www.philsblog.info/2017/05/13/Most-used-Linux-commands.html","excerpt":"","text":"Unzip bz2 keep the original filebzip2 -dk filename.bz2 Find out the current Linux versioncat /etc/*-release ssh-keygen -f id_rsa -p Check hardwarelshw Change java versionupdate-alternatives -config java Change modechmod + xxx.sh ls -lahbzgrep / zgrep / grep “something” xxx.log | xmlprettify | less ( xmlbroswerfy &gt; xxx.html) zgrep RES xxxx.log | grep -v SUCCESS | less (what is grep -v) tail -2 xxx.log | xmlprettifycat xxx.log | xmlprettify bring job to foregroundfg put thread to backgroundbg setup public key on servercat ~/.ssh/id_rsa.pub | ssh user@hostname &#39;cat &gt;&gt; .ssh/authorized_keys&#39; find . -type f -name &#39;*.bz2&#39; -size +0c | while read line; do /bin/bzip2 -dc $line |/usr/bin/xz --best -c &gt; &quot;../xz/${line%.bz2}.xz&quot;; cat /dev/null &gt; $line ; done zgrep something log_name | cut -d &quot;/&quot; -f 2-4 | sort | uniq tar ???? $$ is the process ID (PID) of the script itself. echo ${HOSTNAME%%.*} 两个百分号移除结尾的.后面的东西 “2&gt; /dev/null” 代表忽略掉错误提示信息。 0 – stdin 1 – stdout 2 – stderr change onwership1sudo chown -R phillipxu: /usr/local/lib","categories":[{"name":"Ops","slug":"Ops","permalink":"http://www.philsblog.info/categories/Ops/"},{"name":"Linux","slug":"Ops/Linux","permalink":"http://www.philsblog.info/categories/Ops/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://www.philsblog.info/tags/Linux/"},{"name":"CLI","slug":"CLI","permalink":"http://www.philsblog.info/tags/CLI/"}]},{"title":"Docker machine commands","slug":"Docker-machine-commands","date":"2017-04-16T11:38:06.000Z","updated":"2020-04-26T11:39:40.230Z","comments":true,"path":"2017/04/16/Docker-machine-commands.html","link":"","permalink":"http://www.philsblog.info/2017/04/16/Docker-machine-commands.html","excerpt":"","text":"If you are using a mac, just like a linux, you don’t need docker-machine to run docker-engine locally Basics operation123456docker-machine --versiondocker-machine start defaultdocker-machine stop defaultdocker-machine env defaultdocker-machine lsdocker-machine ssh default Create new dock machine remotely12345docker-machine create -d generic \\ --generic-ip-address=192.168.1.33 \\ --generic-ssh-user=phil \\ --generic-ssh-key ~/.ssh/id_rsa \\ newDockerMachineOfMine","categories":[{"name":"Ops","slug":"Ops","permalink":"http://www.philsblog.info/categories/Ops/"},{"name":"Docker","slug":"Ops/Docker","permalink":"http://www.philsblog.info/categories/Ops/Docker/"}],"tags":[{"name":"CLI","slug":"CLI","permalink":"http://www.philsblog.info/tags/CLI/"},{"name":"Docker","slug":"Docker","permalink":"http://www.philsblog.info/tags/Docker/"}]},{"title":"YAML Syntax","slug":"YAML-Syntax","date":"2017-04-15T23:27:47.000Z","updated":"2020-04-26T11:40:41.789Z","comments":true,"path":"2017/04/16/YAML-Syntax.html","link":"","permalink":"http://www.philsblog.info/2017/04/16/YAML-Syntax.html","excerpt":"","text":"YAML Ain’t Markup LanguageBelow is an example of an invoice expressed via YAML(tm). Structure is shown through indentation (one or more spaces). Sequence items are denoted by a dash, and key value pairs within a map are separated by a colon.1234567891011121314151617181920212223242526272829303132333435363738--- !clarkevans.com/^invoiceinvoice: 34843date : 2001-01-23bill-to: &amp;id001 given : Chris family : Dumars address: lines: | 458 Walkman Dr. Suite #292 city : Royal Oak state : MI postal : 48046ship-to: *id001product: - sku : BL394D quantity : 4 description : Basketball price : 450.00 - sku : BL4438H quantity : 1 description : Super Hoop price : 2392.00tax : 251.42total: 4443.52comments: &gt; Late afternoon is best. Backup contact is Nancy Billsmer @ 338-4338.# load from yaml to HashMap&lt;T, List&lt;Y&gt;&gt;map: T: - Y Y Y","categories":[{"name":"Dev","slug":"Dev","permalink":"http://www.philsblog.info/categories/Dev/"},{"name":"YAML","slug":"Dev/YAML","permalink":"http://www.philsblog.info/categories/Dev/YAML/"}],"tags":[{"name":"Configuration","slug":"Configuration","permalink":"http://www.philsblog.info/tags/Configuration/"},{"name":"YAML","slug":"YAML","permalink":"http://www.philsblog.info/tags/YAML/"}]},{"title":"My Most Used Docker Commands","slug":"My-Most-Used-Docker-Commands","date":"2017-04-14T09:08:01.000Z","updated":"2020-04-26T11:40:33.013Z","comments":true,"path":"2017/04/14/My-Most-Used-Docker-Commands.html","link":"","permalink":"http://www.philsblog.info/2017/04/14/My-Most-Used-Docker-Commands.html","excerpt":"","text":"Check info1234567891011121314151617181920212223docker versiondocker --versiondocker-compose versiondocker-compose --versiondocker ps / docker container lsdocker images / docker image lsdocker inspect [OPTIONS] NAME|ID [NAME|ID...] [flags]docker eventsdocker events --since=&quot;20150720&quot; --until=&quot;20150808&quot;docker port CONTAINER [PRIVATE_PORT[/PROTO]]docker top CONTAINER [ps OPTIONS]docker statsdocker diff CONTAINERdocker infodocker logs (-f &lt;container name or ID&gt;)service docker statussudo service docker start|stop Operations12345678910111213141516171819202122232425262728293031docker logindocker container stop xxxdocker container ls -adocker container rm xxxdocker image lsdocker image rm# search image that has at least 3 likes and can be built automattically# 搜索处收藏数不小于 3 ，并且能够自动化构建的 django 镜像，并且完整显示镜像描述docker search -s 3 --automated --no-trunc djangodocker pull# remove all local imagesdocker rmidocker rmi &apos;docker images -a -q&apos;docker run xxx# remove all local stopped containersdocker system prune# expose portsdocker run -d -p 127.0.0.1:33301:22 centos6-ssh# docker deamon listen to fd and port 2376sudo dockerd -H fd:// -H tcp://0.0.0.0:2376# import docker imagecat centos-6-x86_64.tar.gz | docker import - centos:latest Pull an image example12345678mysqlsudo docker pull mysqlsudo docker run --name first-mysql -p 3306:3306 -e MYSQL\\_ROOT\\_PASSWORD=123456 -d mysqlrun 运行一个容器--name 后面是这个镜像的名称-p 3306:3306 表示在这个容器中使用3306端口(第二个)映射到本机的端口号也为3306(第一个)-d 表示使用守护进程运行，即服务挂在后台docker ps -a Create new user group1234# Add the docker group if it doesn&apos;t already exist.sudo groupadd docker# Don&apos;t forget to log outsudo gpasswd -a $&#123;USER&#125; docker Create a new Docker image1touch Dockerfile add the code following to Dockerfile123FROM alpineCMD [&quot;echo&quot;, &quot;hello world!&quot;] run the following command12docker build .docker run --name test xxxxxxthe id","categories":[{"name":"Ops","slug":"Ops","permalink":"http://www.philsblog.info/categories/Ops/"},{"name":"Docker","slug":"Ops/Docker","permalink":"http://www.philsblog.info/categories/Ops/Docker/"}],"tags":[{"name":"CLI","slug":"CLI","permalink":"http://www.philsblog.info/tags/CLI/"},{"name":"Docker","slug":"Docker","permalink":"http://www.philsblog.info/tags/Docker/"}]},{"title":"2e Systems GmbH","slug":"2e-Systems-GmbH","date":"2017-03-21T22:55:59.000Z","updated":"2020-04-26T10:44:22.491Z","comments":true,"path":"2017/03/22/2e-Systems-GmbH.html","link":"","permalink":"http://www.philsblog.info/2017/03/22/2e-Systems-GmbH.html","excerpt":"","text":"Java Backend developerMelbourne, VIC 2016 - Present Description: 2e Systems is a highly versatile software development company specializing in web, mobile and communications solutions for the airline industry. Leading in systems integration, we empower airlines to apply the best of breed concept in developing their ideal platform. We are fast and flexible, with a dynamic team of experts that can deliver and support innovative, cost-effective solutions. The company was founded in February 2000 near Frankfurt, Germany, and has grown to include offices in Croatia, the UK, Australia, Ireland and the USA. Responsibilities: Development of complex, dynamic applications Analysis of requirements Evaluation and prioritization of activities Interact with project managers and customers Back End part of an international development team Full life-cycle of projects from concept to production Collaborate closely with European colleagues and clients 2e-systems homepage","categories":[{"name":"Work history","slug":"Work-history","permalink":"http://www.philsblog.info/categories/Work-history/"},{"name":"2e Systems GmbH","slug":"Work-history/2e-Systems-GmbH","permalink":"http://www.philsblog.info/categories/Work-history/2e-Systems-GmbH/"}],"tags":[{"name":"Work history","slug":"Work-history","permalink":"http://www.philsblog.info/tags/Work-history/"}]},{"title":"Hibernate Criteria Search","slug":"Hibernate-Criteria-Search","date":"2017-02-23T23:21:04.000Z","updated":"2020-04-26T11:30:29.486Z","comments":true,"path":"2017/02/24/Hibernate-Criteria-Search.html","link":"","permalink":"http://www.philsblog.info/2017/02/24/Hibernate-Criteria-Search.html","excerpt":"","text":"Hibernate Session provides createCriteria() method to create a Criteria object Some samples: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394Criteria cr = session.createCriteria(Employee.class); List results = cr.list(); Criteria cr = session.createCriteria(Employee.class); cr.add(Restrictions.eq(\"salary\", 2000)); List results = cr.list(); Criteria cr = session.createCriteria(Employee.class);// To get records having salary more than 2000cr.add(Restrictions.gt(\"salary\", 2000));// To get records having salary less than 2000cr.add(Restrictions.lt(\"salary\", 2000));// To get records having fistName starting with zaracr.add(Restrictions.like(\"firstName\", \"zara%\"));// Case sensitive form of the above restriction.cr.add(Restrictions.ilike(\"firstName\", \"zara%\"));// To get records having salary in between 1000 and 2000cr.add(Restrictions.between(\"salary\", 1000, 2000));// To check if the given property is nullcr.add(Restrictions.isNull(\"salary\"));// To check if the given property is not nullcr.add(Restrictions.isNotNull(\"salary\"));// To check if the given property is emptycr.add(Restrictions.isEmpty(\"salary\"));// To check if the given property is not emptycr.add(Restrictions.isNotEmpty(\"salary\"));# AND ORCriteria cr = session.createCriteria(Employee.class);Criterion salary = Restrictions.gt(\"salary\", 2000);Criterion name = Restrictions.ilike(\"firstNname\",\"zara%\");// To get records matching with OR condistionsLogicalExpression orExp = Restrictions.or(salary, name);cr.add( orExp );// To get records matching with AND condistionsLogicalExpression andExp = Restrictions.and(salary, name);cr.add( andExp );List results = cr.list();# Max 10 resultCriteria cr = session.createCriteria(Employee.class);cr.setFirstResult(1);cr.setMaxResults(10);List results = cr.list();# SortCriteria cr = session.createCriteria(Employee.class);// To get records having salary more than 2000cr.add(Restrictions.gt(\"salary\", 2000));// To sort records in descening ordercrit.addOrder(Order.desc(\"salary\"));// To sort records in ascending ordercrit.addOrder(Order.asc(\"salary\"));List results = cr.list(); # org.hibernate.criterion.projectionsCriteria cr = session.createCriteria(Employee.class);// To get total row count.cr.setProjection(Projections.rowCount());// To get average of a property.cr.setProjection(Projections.avg(\"salary\"));// To get distinct count of a property.cr.setProjection(Projections.countDistinct(\"firstName\"));// To get maximum of a property.cr.setProjection(Projections.max(\"salary\"));// To get minimum of a property.cr.setProjection(Projections.min(\"salary\"));// To get sum of a property.cr.setProjection(Projections.sum(\"salary\"));","categories":[{"name":"Dev","slug":"Dev","permalink":"http://www.philsblog.info/categories/Dev/"},{"name":"Hibernate","slug":"Dev/Hibernate","permalink":"http://www.philsblog.info/categories/Dev/Hibernate/"}],"tags":[{"name":"Hibernate","slug":"Hibernate","permalink":"http://www.philsblog.info/tags/Hibernate/"},{"name":"JPA","slug":"JPA","permalink":"http://www.philsblog.info/tags/JPA/"}]},{"title":"Hibernate SQL","slug":"Hibernate-SQL","date":"2017-02-23T05:35:34.000Z","updated":"2020-04-26T11:30:41.556Z","comments":true,"path":"2017/02/23/Hibernate-SQL.html","link":"","permalink":"http://www.philsblog.info/2017/02/23/Hibernate-SQL.html","excerpt":"","text":"Run SQL123456789101112131415String sql = \"SELECT first_name, salary FROM EMPLOYEE\";SQLQuery query = session.createSQLQuery(sql);query.setResultTransformer(Criteria.ALIAS_TO_ENTITY_MAP);List results = query.list();String sql = \"SELECT * FROM EMPLOYEE\";SQLQuery query = session.createSQLQuery(sql);query.addEntity(Employee.class);List results = query.list(); String sql = \"SELECT * FROM EMPLOYEE WHERE id = :employee_id\";SQLQuery query = session.createSQLQuery(sql);query.addEntity(Employee.class);query.setParameter(\"employee_id\", 10);List results = query.list();","categories":[{"name":"Dev","slug":"Dev","permalink":"http://www.philsblog.info/categories/Dev/"},{"name":"Hibernate","slug":"Dev/Hibernate","permalink":"http://www.philsblog.info/categories/Dev/Hibernate/"}],"tags":[{"name":"Hibernate","slug":"Hibernate","permalink":"http://www.philsblog.info/tags/Hibernate/"},{"name":"JPA","slug":"JPA","permalink":"http://www.philsblog.info/tags/JPA/"}]},{"title":"Hibernate Query","slug":"Hibernate-Query","date":"2017-02-23T04:45:04.000Z","updated":"2020-04-26T11:30:35.431Z","comments":true,"path":"2017/02/23/Hibernate-Query.html","link":"","permalink":"http://www.philsblog.info/2017/02/23/Hibernate-Query.html","excerpt":"","text":"Hibernate query languageExample 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283# FROMString hql = \"FROM Employee\";Query query = session.createQuery(hql);List results = query.list();# To specify the classString hql = \"FROM com.hibernatebook.criteria.Employee\";Query query = session.createQuery(hql);List results = query.list();# ASString hql = \"FROM Employee AS E\";Query query = session.createQuery(hql);List results = query.list();# SELECTString hql = \"SELECT E.firstName FROM Employee E\";Query query = session.createQuery(hql);List results = query.list();# WHEREString hql = \"FROM Employee E WHERE E.id = 10\";Query query = session.createQuery(hql);List results = query.list();# ORDER BYString hql = \"FROM Employee E WHERE E.id &gt; 10 ORDER BY E.salary DESC\";Query query = session.createQuery(hql);List results = query.list();# ORDER on multiple columnString hql = \"FROM Employee E WHERE E.id &gt; 10 \" + \"ORDER BY E.firstName DESC, E.salary DESC \";Query query = session.createQuery(hql);List results = query.list();# GROUP BYString hql = \"SELECT SUM(E.salary), E.firtName FROM Employee E \" + \"GROUP BY E.firstName\";Query query = session.createQuery(hql);List results = query.list();# USE ParametersString hql = \"FROM Employee E WHERE E.id = :employee_id\";Query query = session.createQuery(hql);query.setParameter(\"employee_id\",10);List results = query.list();# UPDATEString hql = \"UPDATE Employee set salary = :salary \" + \"WHERE id = :employee_id\";Query query = session.createQuery(hql);query.setParameter(\"salary\", 1000);query.setParameter(\"employee_id\", 10);int result = query.executeUpdate();System.out.println(\"Rows affected: \" + result);# DELETEString hql = \"DELETE FROM Employee \" + \"WHERE id = :employee_id\";Query query = session.createQuery(hql);query.setParameter(\"employee_id\", 10);int result = query.executeUpdate();System.out.println(\"Rows affected: \" + result);# INSERTString hql = \"INSERT INTO Employee(firstName, lastName, salary)\" + \"SELECT firstName, lastName, salary FROM old_employee\";Query query = session.createQuery(hql);int result = query.executeUpdate();System.out.println(\"Rows affected: \" + result);# distinctString hql = \"SELECT count(distinct E.firstName) FROM Employee E\";Query query = session.createQuery(hql);List results = query.list();# Max resultString hql = \"FROM Employee\";Query query = session.createQuery(hql);query.setFirstResult(1);query.setMaxResults(10);List results = query.list();","categories":[{"name":"Dev","slug":"Dev","permalink":"http://www.philsblog.info/categories/Dev/"},{"name":"Hibernate","slug":"Dev/Hibernate","permalink":"http://www.philsblog.info/categories/Dev/Hibernate/"}],"tags":[{"name":"Hibernate","slug":"Hibernate","permalink":"http://www.philsblog.info/tags/Hibernate/"},{"name":"JPA","slug":"JPA","permalink":"http://www.philsblog.info/tags/JPA/"}]},{"title":"Hibernate Mapping","slug":"Hibernate-Mapping","date":"2017-02-21T22:34:09.000Z","updated":"2020-04-26T11:30:32.803Z","comments":true,"path":"2017/02/22/Hibernate-Mapping.html","link":"","permalink":"http://www.philsblog.info/2017/02/22/Hibernate-Mapping.html","excerpt":"","text":"A mapping example using xml123456789101112131415161718&lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;&lt;!DOCTYPE hibernate-mapping PUBLIC \"-//Hibernate/Hibernate Mapping DTD//EN\" \"http://www.hibernate.org/dtd/hibernate-mapping-3.0.dtd\"&gt; &lt;hibernate-mapping&gt; &lt;class name=\"Employee\" table=\"EMPLOYEE\"&gt; &lt;meta attribute=\"class-description\"&gt; This class contains the employee detail. &lt;/meta&gt; &lt;id name=\"id\" type=\"int\" column=\"id\"&gt; &lt;generator class=\"native\"/&gt; &lt;/id&gt; &lt;property name=\"firstName\" column=\"first_name\" type=\"string\"/&gt; &lt;property name=\"lastName\" column=\"last_name\" type=\"string\"/&gt; &lt;property name=\"salary\" column=\"salary\" type=\"int\"/&gt; &lt;/class&gt;&lt;/hibernate-mapping&gt; The file name should be .hbm.xml Mapping typePrimitive types Mapping Type Java Type ANSI SQL type integer int/java.lang.Integer INTEGER long long/java.lang.Long BIGINT short short/java.lang.Short SMALLINT float float/java.lang.Float FLOAT double double/java.lang.Double DOUBLE big_decimal java.math.BigDecimal NUMERIC character java.lang.String CHAR(1) string java.lang.String VARCHAR byte byte/java.lang.Byte TINYINT boolean boolean/java.lang.Boolean BIT yes/no boolean/java.lang.Boolean CHAR(1) (‘Y’ or ‘N’) true/false boolean/java.lang.Boolean CHAR(1) (‘T’ or ‘F’) Date types Mapping Type Java Type ANSI SQL type date java.util.Date/java.sql.Date DATE time java.util.Date/java.sql.Time TIME timestamp java.util.Date/java.sql.Timestamp TIMESTAMP calendar java.util.Calendar TIMESTAMP calendar_date java.util.Calendar DATE Binary and BLOB types Mapping Type Java Type ANSI SQL type binary byte[] VARBINARY (or BLOB) text java.lang.String CLOB serializable any Java class that implements java.io.Serializable VARBINARY (or BLOB) clob java.sql.Clob CLOB blob java.sql.Blob BLOB JDK arelated types Mapping Type Java Type ANSI SQL type class java.lang.Class VARCHAR locale java.util.Locale VARCHAR timezone java.util.TimeZone VARCHAR currency java.util.Currency VARCHAR Annotations@Entity @Table @id @GeneratedValue @Column An example1234567891011121314151617181920212223242526272829303132333435363738394041424344import javax.persistence.*;@Entity@Table(name = \"EMPLOYEE\")public class Employee &#123; @Id @GeneratedValue @Column(name = \"id\") private int id; @Column(name = \"first_name\") private String firstName; @Column(name = \"last_name\") private String lastName; @Column(name = \"salary\") private int salary; public Employee() &#123;&#125; public int getId() &#123; return id; &#125; public void setId( int id ) &#123; this.id = id; &#125; public String getFirstName() &#123; return firstName; &#125; public void setFirstName( String first_name ) &#123; this.firstName = first_name; &#125; public String getLastName() &#123; return lastName; &#125; public void setLastName( String last_name ) &#123; this.lastName = last_name; &#125; public int getSalary() &#123; return salary; &#125; public void setSalary( int salary ) &#123; this.salary = salary; &#125;&#125;","categories":[{"name":"Dev","slug":"Dev","permalink":"http://www.philsblog.info/categories/Dev/"},{"name":"Hibernate","slug":"Dev/Hibernate","permalink":"http://www.philsblog.info/categories/Dev/Hibernate/"}],"tags":[{"name":"Hibernate","slug":"Hibernate","permalink":"http://www.philsblog.info/tags/Hibernate/"},{"name":"JPA","slug":"JPA","permalink":"http://www.philsblog.info/tags/JPA/"}]},{"title":"Hibernate Session","slug":"Hibernate-Session","date":"2017-02-19T13:25:55.000Z","updated":"2020-04-26T11:30:39.203Z","comments":true,"path":"2017/02/20/Hibernate-Session.html","link":"","permalink":"http://www.philsblog.info/2017/02/20/Hibernate-Session.html","excerpt":"","text":"States: Transient Persistent Detached A typical sample of tranction1234567891011121314Session session = factory.openSession();Transaction tx = null;try &#123; tx = session.beginTransaction(); // do some work ... tx.commit();&#125;catch (Exception e) &#123; if (tx!=null) tx.rollback(); e.printStackTrace(); &#125;finally &#123; session.close();&#125; Session interface Method Description Transaction beginTransaction() 开始工作单位，并返回关联事务对象。 void cancelQuery() 取消当前的查询执行。 void clear() 完全清除该会话。 Connection close() 通过释放和清理 JDBC 连接以结束该会话。 Criteria createCriteria(Class persistentClass) 为给定的实体类或实体类的超类创建一个新的 Criteria 实例。 Criteria createCriteria(String entityName) 为给定的实体名称创建一个新的 Criteria 实例。 Serializable getIdentifier(Object object) 返回与给定实体相关联的会话的标识符值。 Query createFilter(Object collection, String queryString) 为给定的集合和过滤字符创建查询的新实例。 Query createQuery(String queryString) 为给定的 HQL 查询字符创建查询的新实例。 SQLQuery createSQLQuery(String queryString) 为给定的 SQL 查询字符串创建 SQLQuery 的新实例。 void delete(Object object) 从数据存储中删除持久化实例。 void delete(String entityName, Object object) 从数据存储中删除持久化实例。 Session get(String entityName, Serializable id) 返回给定命名的且带有给定标识符或 null 的持久化实例（若无该种持久化实例）。 SessionFactory getSessionFactory() 获取创建该会话的 session 工厂。 void refresh(Object object) 从基本数据库中重新读取给定实例的状态。 Transaction getTransaction() 获取与该 session 关联的事务实例。 boolean isConnected() 检查当前 session 是否连接。 boolean isDirty() 该 session 中是否包含必须与数据库同步的变化？ boolean isOpen() 检查该 session 是否仍处于开启状态。 Serializable save(Object object) 先分配一个生成的标识，以保持给定的瞬时状态实例。 void saveOrUpdate(Object object) 保存（对象）或更新（对象）给定的实例。 void update(Object object) 更新带有标识符且是给定的处于脱管状态的实例的持久化实例。 void update(String entityName, Object object) 更新带有标识符且是给定的处于脱管状态的实例的持久化实例。","categories":[{"name":"Dev","slug":"Dev","permalink":"http://www.philsblog.info/categories/Dev/"},{"name":"Hibernate","slug":"Dev/Hibernate","permalink":"http://www.philsblog.info/categories/Dev/Hibernate/"}],"tags":[{"name":"Hibernate","slug":"Hibernate","permalink":"http://www.philsblog.info/tags/Hibernate/"},{"name":"JPA","slug":"JPA","permalink":"http://www.philsblog.info/tags/JPA/"}]},{"title":"Hibernate Configuration","slug":"Hibernate-Configuration","date":"2017-02-17T13:07:41.000Z","updated":"2020-04-26T11:30:27.424Z","comments":true,"path":"2017/02/18/Hibernate-Configuration.html","link":"","permalink":"http://www.philsblog.info/2017/02/18/Hibernate-Configuration.html","excerpt":"","text":"Hibernate ORM (Hibernate in short) is an object-relational mapping tool for the Java programming language. It provides a framework for mapping an object-oriented domain model to a relational database. Hibernate handles object-relational impedance mismatch problems by replacing direct, persistent database accesses with high-level object handling functions. Diagram SessionFactorySessionTransactionQueryCriteria Dependency Library Description dom4j Xml Parsing Xalan XSLT processing Xerces The Xerces Java Parser cglib Java Code Generator log4j Logging Commons logger, email SLF4J logging abstract Properties Name Description hibernate.dialet Specify the SQL language hibernate.connection.driver_class JDBC driver hibernate.connection.url DB instance JDBC URL hibernate.connection.username DB username hibernate.connection.password DB password hibernate.connection.pool_size The size of the connection pool hibernate.connection.autocommit Allow JDBC to autocommit If use JNDI|Name|Description||:-|:-||hibernate.connection.datasource|JNDI name in your server env||hibernate.jndi.class|InitialContext Class||hibernate.jndi.|Java properties in InitialContext||hibernate.jndi.url|JNDI URL| A sample of hibernate.cfg.xml1234567891011121314151617181920212223242526272829&lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;&lt;!DOCTYPE hibernate-configuration SYSTEM \"http://www.hibernate.org/dtd/hibernate-configuration-3.0.dtd\"&gt;&lt;hibernate-configuration&gt; &lt;session-factory&gt; &lt;property name=\"hibernate.dialect\"&gt; org.hibernate.dialect.MySQLDialect &lt;/property&gt; &lt;property name=\"hibernate.connection.driver_class\"&gt; com.mysql.jdbc.Driver &lt;/property&gt; &lt;!-- Assume test is the database name --&gt; &lt;property name=\"hibernate.connection.url\"&gt; jdbc:mysql://localhost/test &lt;/property&gt; &lt;property name=\"hibernate.connection.username\"&gt; root &lt;/property&gt; &lt;property name=\"hibernate.connection.password\"&gt; root123 &lt;/property&gt; &lt;!-- List of XML mapping files --&gt; &lt;mapping resource=\"Employee.hbm.xml\"/&gt;&lt;/session-factory&gt;&lt;/hibernate-configuration&gt;","categories":[{"name":"Dev","slug":"Dev","permalink":"http://www.philsblog.info/categories/Dev/"},{"name":"Hibernate","slug":"Dev/Hibernate","permalink":"http://www.philsblog.info/categories/Dev/Hibernate/"}],"tags":[{"name":"Configuration","slug":"Configuration","permalink":"http://www.philsblog.info/tags/Configuration/"},{"name":"Hibernate","slug":"Hibernate","permalink":"http://www.philsblog.info/tags/Hibernate/"},{"name":"JPA","slug":"JPA","permalink":"http://www.philsblog.info/tags/JPA/"}]},{"title":"Git CLI","slug":"Git-CLI","date":"2017-02-15T09:53:41.000Z","updated":"2020-04-26T11:39:02.419Z","comments":true,"path":"2017/02/15/Git-CLI.html","link":"","permalink":"http://www.philsblog.info/2017/02/15/Git-CLI.html","excerpt":"","text":"Commands Sequence Git Basics name Description master default development branch origin default upstream repository HEAD current branch HEAD^ parent of HEAD HEAD~4 the great-great grandparent of HEAD CreateFrom existing data123cd ~/projects/myprojectgit initgit add . From existing repo123git clone ~/existing/repo ~/new/repogit clone git://host.org/project.gitgit clone ssh://you@host.org/proj.git Show Description Command Files changed in working directory git status Changes to tracked files git diff What changed between $ID1 and $ID2 git diff $id1 $id2 History of changes git log History of changes for file with diffs git log -p $file $dir/ec/tory/ Who changed what and when in a file git blame $file A commit identified by $ID git show $id A specific file from a specific $ID git show $id:$file All local branches git branch Revert Description Command Return to the last committed state git reset –hard Revert the last commit git revert HEAD Revert specific commit git revert $id Fix the last commit git commit -a –amend Checkout the $id version of a file git checkout $id $file Branch Description Command Switch to the $id branch git checkout $id Merge branch1 into branch2 git checkout $branch2git merge $branch1 Create branch named $branch bansed onthe HEAD git branch $branch Create branch $new_branch based on branch$other and switch to it git checkout -b $new_branch $other Delete branch $branch git branch -d $branch Update Description Command Fetch latest changes from origin(but this does not merge them). git fetch Pull latest changes from origin(does a fetch followed by a merge) git pull Apply a patch that some sent you git am -3 patch.mbox Push|Commit all your local changes|git commit -a||Prepare a patch for other developers|git format-patch origin||Push changes to origin|git push||Mark a version / milestone|git tag v1.0| Useful CommandsTo view the merge conclicts1234git diffgit diff --base $filegit diff --ours $filegit diff --theirs $file To discard conflicting patch12git reset --hardgit rebase --skip After resolving conflicts, merge with12git add $conflicting_filegit rebase --continue Finding regressions123456git bisect startgit bisect good $idgit bisect bad $idgit bisect bad/goodgit bisect visualizegit bisect reset Check for errors and cleanup repository12git fsckgit gc --prune Search working directory for foo()1git grep &quot;foo()&quot;","categories":[{"name":"Dev","slug":"Dev","permalink":"http://www.philsblog.info/categories/Dev/"},{"name":"Version Control","slug":"Dev/Version-Control","permalink":"http://www.philsblog.info/categories/Dev/Version-Control/"}],"tags":[{"name":"Git","slug":"Git","permalink":"http://www.philsblog.info/tags/Git/"},{"name":"Version Control","slug":"Version-Control","permalink":"http://www.philsblog.info/tags/Version-Control/"}]},{"title":"Java Currency","slug":"Java-Currency","date":"2017-01-27T09:31:02.000Z","updated":"2020-04-26T11:29:51.325Z","comments":true,"path":"2017/01/27/Java-Currency.html","link":"","permalink":"http://www.philsblog.info/2017/01/27/Java-Currency.html","excerpt":"","text":"Use Thread There are three ways of using Thread: Implementes Runnable interface Implementes Callable interface Extends Thread class Implemeting Runnable and Callable does not create a new real thead, it only creates a new task and will be invoked by Thread. So i.e. Task is Thread driven. Implement Runnable interfaceImplement run() method and called by start() from a thread 12345678910public class MyRunnable implements Runnable &#123; public void run() &#123; // ... &#125; public static void main(String[] args) &#123; MyRunnable instance = new MyRunnable(); Tread thread = new Thread(instance); thread.start(); &#125;&#125; Implement Callable interfaceImplementing call() method will have return value, the value will be encapsulated in FutureTask 123456789101112public class MyCallable implements Callable&lt;Integer&gt; &#123; public Integer call() &#123; // ... &#125; public static void main(String[] args) &#123; MyCallable mc = new MyCallable(); FutureTask&lt;Integer&gt; ft = new FutureTask&lt;&gt;(mc); Thread thread = new Thread(ft); thread.start(); System.out.println(ft.get()); &#125;&#125; Extend Thread classImplement run() methond and call start by thread123456789public class MyThread extends Thread &#123; public void run() &#123; // ... &#125; public static void main(String[] args) &#123; MyThread mt = new MyThread(); mt.start(); &#125;&#125; Implementing Interface VS Extending Thread ClassImplementing interface is better, because: Java doesn’t support multi-extend, so if a class extends Thread, then it cannot extend other classes. Extending Thread has too much overhead Thread mechanismThread.sleep(millisec) will put current thread to sleep. Can use TimeUnit.TILLISECONDS.sleep(millisec) instead. sleep() will through interruptedException. Exception cannot cross threads to main(), so it has to be handled in the local thread. 123456789public void run() &#123; try &#123; // ... Thread.sleep(1000); // ... &#125; catch (InterruptedException e) &#123; System.err.println(e); &#125;&#125; Thread.yield() means the current thead has finished its job, it can hand over to other threads. Calling join() from another thread would cause the suspension of the current thread, until the target thread finish. deamon thread is the Program Runtime thread service in the background. Use setDaemon() to put a thread in the background. Kill a threadStopA thread can be stop by the following: Call Thread.sleep() method Call wait() to suspend thread untill notify() or notifyAll() is called, Or the signal() signalAll() in java.util.concurrent Until some I/O finish Try to call the sync control method on an object, cannot use object lock, since the thread has got the lock. Interrupt","categories":[{"name":"Dev","slug":"Dev","permalink":"http://www.philsblog.info/categories/Dev/"},{"name":"Java","slug":"Dev/Java","permalink":"http://www.philsblog.info/categories/Dev/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://www.philsblog.info/tags/Java/"}]},{"title":"Git vs Mercurial: Why Git? Why Mercurial?","slug":"Git-vs-Mercurial-Why-Git-Why-Mercurial","date":"2016-12-04T06:01:24.000Z","updated":"2020-04-26T11:39:05.065Z","comments":true,"path":"2016/12/04/Git-vs-Mercurial-Why-Git-Why-Mercurial.html","link":"","permalink":"http://www.philsblog.info/2016/12/04/Git-vs-Mercurial-Why-Git-Why-Mercurial.html","excerpt":"","text":"I used to use SVN and Git, never used CVS and never heard about Mercurial. Okay, recently I joined a new company, and they use Mercurial hg, so I read some articles about it. Since Git is very popular nowadays, I would like to compare Mercurial to Git, to find what are different between them, what are the same? And which to choose. Git vs Mercurial: Why Git?Mercurial vs Git: Why Mercurial? First, I find these two articles, one prefers Git and the other likes Mercurial, but the tricky thing is both of them believe they pick the right one with a safer history control. If we have a closer look, we’d know that both of them are well in a given situation. According to Charles O’Farrel, Git never deletes or modify something, the only thing you can do is commit, making a new object without eliminating the old one, keep the old one for 30 days then delete them if they do not have a reference. Mercurial do not have such features unless installing extensions, which have a few more steps to do than Git. On the contrary, Steve Losh claims that it is risky to give users easy access to destructive commands, since they may not fully understand the commands, and sometimes, the git commands which have more arguments are a bit more complicated than Hg’s. Besides, you might lose the history completely in 30 days, but you will never lose them in Mercurial since most of the extensions will permanently back up any changesets that they destroy into a bundle, and these bundles will not be garbage collected, so “you don’t have to worry about your version control system eating your data”. As mention above, Mercurial has more specific commands for each purpose, Git use less command with different arguments, the author in the second article prefers Mercurial, due to “ each command should do one thing and do it well”, which follows the UNIX philosophy.","categories":[{"name":"Dev","slug":"Dev","permalink":"http://www.philsblog.info/categories/Dev/"},{"name":"Version Control","slug":"Dev/Version-Control","permalink":"http://www.philsblog.info/categories/Dev/Version-Control/"}],"tags":[{"name":"Git","slug":"Git","permalink":"http://www.philsblog.info/tags/Git/"},{"name":"Version Control","slug":"Version-Control","permalink":"http://www.philsblog.info/tags/Version-Control/"},{"name":"hg","slug":"hg","permalink":"http://www.philsblog.info/tags/hg/"}]},{"title":"Mercurial hg CLI","slug":"Mercurial-hg-CLI","date":"2016-12-04T05:57:39.000Z","updated":"2020-04-26T11:39:07.757Z","comments":true,"path":"2016/12/04/Mercurial-hg-CLI.html","link":"","permalink":"http://www.philsblog.info/2016/12/04/Mercurial-hg-CLI.html","excerpt":"","text":"Synopsis1hg command [option]... [argument]... addadd the specified files on the next commit:1hg add [OPTION]… [FILE]… addremoveadd all new files, delete all missing files: 1hg addremove [OPTION]... [FILE]... Add all new files and remove all missing files from the repository. annotateshow changeset information by line for each file:1hg annotate [-r REV] [-f] [-a] [-u] [-d] [-n] [-c] [-l] FILE... List changes in files, showing the revision id responsible for each line. archivecreate an unversioned archive of a repository revision: 1hg archive [OPTION]... DEST By default, the revision used is the parent of the working directory; use -r/–rev to specify a different revision. backoutreverse effect of earlier changeset:1hg backout [OPTION]... [-r] REV Prepare a new changeset with the effect of REV undone in the current working directory. If no conflicts were encountered, it will be committed immediately. brancheslist repository named branches: 1hg branches [-c] List the repository’s named branches, indicating which ones are inactive. If -c/–closed is specified, also list branches which have been marked closed catoutput the current or given revision of files:1hg cat [OPTION]... FILE... Print the specified files as they were at the given revision. If no revision is given, the parent of the working directory is used. clonemake a copy of an existing repository: 1hg clone [OPTION]... SOURCE [DEST] Create a copy of an existing repository in a new directory. commitcommit the specified files or all outstanding changes: 1hg commit [OPTION]... [FILE]... Commit changes to the given files into the repository. Unlike a centralized SCM, this operation is a local operation. configshow combined config settings from all hgrc files:1hg config [-u] [NAME]... With no arguments, print names and values of all config items.With one argument of the form section.name, print just the value of that config item.With multiple arguments, print names and values of all config items with matching section names. copymark files as copied for the next commit: 1hg copy [OPTION]... [SOURCE]... DEST Mark dest as having copies of source files. If dest is a directory, copies are put in that directory. If dest is a file, the source must be a single file. diffdiff repository (or selected files): 1hg diff [OPTION]... ([-c REV] | [-r REV1 [-r REV2]]) [FILE]... Show differences between revisions for the specified files.Differences between files are shown using the unified diff format. exportdump the header and diffs for one or more changesets: 1hg export [OPTION]... [-o OUTFILESPEC] [-r] [REV]... Print the changeset header and diffs for one or more revisions. If no revision is given, the parent of the working directory is used.","categories":[{"name":"Dev","slug":"Dev","permalink":"http://www.philsblog.info/categories/Dev/"},{"name":"Version Control","slug":"Dev/Version-Control","permalink":"http://www.philsblog.info/categories/Dev/Version-Control/"}],"tags":[{"name":"CLI","slug":"CLI","permalink":"http://www.philsblog.info/tags/CLI/"},{"name":"Version Control","slug":"Version-Control","permalink":"http://www.philsblog.info/tags/Version-Control/"},{"name":"hg","slug":"hg","permalink":"http://www.philsblog.info/tags/hg/"}]},{"title":"Another neat tool -- automate your build process using Java and Ant","slug":"Another-neat-tool-automate-your-build-process-using-Java-and-Ant","date":"2016-12-03T05:47:06.000Z","updated":"2020-04-26T11:38:08.404Z","comments":true,"path":"2016/12/03/Another-neat-tool-automate-your-build-process-using-Java-and-Ant.html","link":"","permalink":"http://www.philsblog.info/2016/12/03/Another-neat-tool-automate-your-build-process-using-Java-and-Ant.html","excerpt":"","text":"Apache Ant is a Java library and command-line tool whose mission is to drive processes described in build files as targets and extension points dependent upon each other. The main known usage of Ant is the build of Java applications. — ant.apache.org ANT is similar to Make, the most immediately noticeable difference between Ant and Make is that Ant uses XML to describe the build process and its dependencies, whereas Make uses Makefile format. By default the XML file is named build.xml. Ant is an Apache project. It is open source software, and is released under the Apache License. — Wikipedia A build file looks like this: 1234567891011121314151617181920&lt;project&gt; &lt;target name=\"clean\"&gt; &lt;delete dir=\"build\"/&gt; &lt;/target&gt; &lt;target name=\"compile\"&gt; &lt;mkdir dir=\"build/classes\"/&gt; &lt;javac srcdir=\"src\" destdir=\"build/classes\"/&gt; &lt;/target&gt; &lt;target name=\"jar\"&gt; &lt;mkdir dir=\"build/jar\"/&gt; &lt;jar destfile=\"build/jar/HelloWorld.jar\" basedir=\"build/classes\"&gt; &lt;manifest&gt; &lt;attribute name=\"Main-Class\" value=\"oata.HelloWorld\"/&gt; &lt;/manifest&gt; &lt;/jar&gt; &lt;/target&gt; &lt;target name=\"run\"&gt; &lt;java jar=\"build/jar/HelloWorld.jar\" fork=\"true\"/&gt; &lt;/target&gt;&lt;/project&gt; Now compile, package and run the application via 123ant compileant jarant run or shorter with1ant compile jar run We can build the same project with pure java command12345678910111213md build\\classesjavac -sourcepath src -d build\\classes src\\oata\\HelloWorld.javaecho Main-Class: oata.HelloWorld&gt;mfmd build\\jarjar cfm build\\jar\\HelloWorld.jar mf -C build\\classes .java -jar build\\jar\\HelloWorld.jar Enhance the build file123456789101112131415161718192021222324252627&lt;project name=\"HelloWorld\" basedir=\".\" default=\"main\"&gt; &lt;property name=\"src.dir\" value=\"src\"/&gt; &lt;property name=\"build.dir\" value=\"build\"/&gt; &lt;property name=\"classes.dir\" value=\"$&#123;build.dir&#125;/classes\"/&gt; &lt;property name=\"jar.dir\" value=\"$&#123;build.dir&#125;/jar\"/&gt; &lt;property name=\"main-class\" value=\"oata.HelloWorld\"/&gt; &lt;target name=\"clean\"&gt; &lt;delete dir=\"$&#123;build.dir&#125;\"/&gt; &lt;/target&gt; &lt;target name=\"compile\"&gt; &lt;mkdir dir=\"$&#123;classes.dir&#125;\"/&gt; &lt;javac srcdir=\"$&#123;src.dir&#125;\" destdir=\"$&#123;classes.dir&#125;\"/&gt; &lt;/target&gt; &lt;target name=\"jar\" depends=\"compile\"&gt; &lt;mkdir dir=\"$&#123;jar.dir&#125;\"/&gt; &lt;jar destfile=\"$&#123;jar.dir&#125;/$&#123;ant.project.name&#125;.jar\" basedir=\"$&#123;classes.dir&#125;\"&gt; &lt;manifest&gt; &lt;attribute name=\"Main-Class\" value=\"$&#123;main-class&#125;\"/&gt; &lt;/manifest&gt; &lt;/jar&gt; &lt;/target&gt; &lt;target name=\"run\" depends=\"jar\"&gt; &lt;java jar=\"$&#123;jar.dir&#125;/$&#123;ant.project.name&#125;.jar\" fork=\"true\"/&gt; &lt;/target&gt; &lt;target name=\"clean-build\" depends=\"clean,jar\"/&gt; &lt;target name=\"main\" depends=\"clean,run\"/&gt;&lt;/project&gt; just do an ant and you will get123456789101112Buildfile: build.xmlclean:compile: [mkdir] Created dir: C:\\...\\build\\classes [javac] Compiling 1 source file to C:\\...\\build\\classesjar: [mkdir] Created dir: C:\\...\\build\\jar [jar] Building jar: C:\\...\\build\\jar\\HelloWorld.jarrun: [java] Hello Worldmain:BUILD SUCCESSFUL","categories":[{"name":"Dev","slug":"Dev","permalink":"http://www.philsblog.info/categories/Dev/"},{"name":"ANT","slug":"Dev/ANT","permalink":"http://www.philsblog.info/categories/Dev/ANT/"}],"tags":[{"name":"Tools","slug":"Tools","permalink":"http://www.philsblog.info/tags/Tools/"},{"name":"ANT","slug":"ANT","permalink":"http://www.philsblog.info/tags/ANT/"}]},{"title":"Useful commands that are built in the Ant distribution","slug":"Useful-commands-that-are-built-in-the-Ant-distribution","date":"2016-12-03T05:38:03.000Z","updated":"2020-04-26T11:38:03.014Z","comments":true,"path":"2016/12/03/Useful-commands-that-are-built-in-the-Ant-distribution.html","link":"","permalink":"http://www.philsblog.info/2016/12/03/Useful-commands-that-are-built-in-the-Ant-distribution.html","excerpt":"","text":"Here are some commands Command Description Ant Used to execute another ant process from within the current one. Cotydir Used to copy an entire directory. Copyfile Used to copy a single file. Cvs Handles packages/modules retrieved from a CVS repository. Delete Deletes either a single file or all files in a specified directory and its sub-directories. Deltree Deletes a directory with all its files and subdirectories. Exec Executes a system command. When the os attribute is specified, then the command is only executed when Ant is run on one of the specified operating systems. Get Gets a file from an URL. Jar Jars a set of files. Java Executes a Java class within the running (Ant) VM or forks another VM if specified. Javac Compiles a source tree within the running (Ant) VM. Javadoc/javadoc2 Generates code documentation using the javadoc tool. Mkdir Makes a directory. Property Sets a property (by name and value), or set of properties (from file or resource) in the project. Rmic Runs the rmic compiler for a certain class. Tstamp Sets the DSTAMP, TSTAMP, and TODAY properties in the current project. Style Processes a set of documents via XSLT.","categories":[{"name":"Dev","slug":"Dev","permalink":"http://www.philsblog.info/categories/Dev/"},{"name":"ANT","slug":"Dev/ANT","permalink":"http://www.philsblog.info/categories/Dev/ANT/"}],"tags":[{"name":"ANT","slug":"ANT","permalink":"http://www.philsblog.info/tags/ANT/"},{"name":"TOOLS","slug":"TOOLS","permalink":"http://www.philsblog.info/tags/TOOLS/"}]},{"title":"MySQL bottleneck analysis","slug":"MySQL-bottleneck-analysis","date":"2016-11-14T05:29:35.000Z","updated":"2020-04-26T11:37:32.824Z","comments":true,"path":"2016/11/14/MySQL-bottleneck-analysis.html","link":"","permalink":"http://www.philsblog.info/2016/11/14/MySQL-bottleneck-analysis.html","excerpt":"","text":"Find bottleneck IOPS Input/output operations per second (IOPS, pronounced eye-ops) is a performance measurement used to characterize computer storage devices like hard disk drives (HDD), solid state drives (SSD), and storage area networks (SAN). – Wikipedia’ IOAny system that involves with storage has IO bottleneck. Databases are just some data files, so interacting with a database is an IO operation. There are two situations when we are using MySQL, OLTP and OLAP. They is a difference regarding the aspect of using MySQL. OLTP: On-line Transaction Processing, business, focus on concurrency.OLAP: On-line Analytical Processing, log and data warehousing, focus on throughput. There will be an option when you install MySQL on Windows. CPUBe careful when using order and group by, since both of them consume computing resources, which means more CPU usage. NetworkIn a distributed environment, each node between databases is more likely to become a bottleneck. When bottleneck detected1) Hardware, OS, Network IO ( add more memory; improve IO performance) CPU (Multi-core) Network (MPLS）","categories":[{"name":"Dev","slug":"Dev","permalink":"http://www.philsblog.info/categories/Dev/"},{"name":"SQL","slug":"Dev/SQL","permalink":"http://www.philsblog.info/categories/Dev/SQL/"}],"tags":[{"name":"SQL","slug":"SQL","permalink":"http://www.philsblog.info/tags/SQL/"},{"name":"Tech","slug":"Tech","permalink":"http://www.philsblog.info/tags/Tech/"}]},{"title":"JasperReport","slug":"JasperReport","date":"2016-11-13T05:20:05.000Z","updated":"2020-04-26T11:30:07.465Z","comments":true,"path":"2016/11/13/JasperReport.html","link":"","permalink":"http://www.philsblog.info/2016/11/13/JasperReport.html","excerpt":"","text":"JasperReports is an open source Java reporting tool that can write to a variety of targets, such as: screen, a printer, into PDF, HTML, Microsoft Excel, RTF, ODT, Comma-separated values or XML files. It can be used in Java-enabled applications, including Java EE or web applications, to generate dynamic content. –Wikipedia Download it, unzip it. JasperReport Lib cannot run alone and we don’t really need to install it, we only need to cp it to classpath directory with all other jar files. JasperReport using AWT generate report, if using terminal and terminal only, then JasperReport is not a good choice. Generate report 123456789101112131415161718192021222324252627282930package jasperreport.javabean; import java.util.Date; public class User &#123; private String username; private Date birthday; public User(String username, Date birthday) &#123; this.username = username; this.birthday = birthday; &#125; public String getUsername() &#123; return username; &#125; public void setUsername(String username) &#123; this.username = username; &#125; public Date getBirthday() &#123; return birthday; &#125; public void setBirthday(Date birthday) &#123; this.birthday = birthday; &#125; &#125; 1234567891011121314151617181920212223242526272829303132333435363738394041424344package jasperreport.javabean; import java.io.InputStream; import java.util.ArrayList; import java.util.Date; import java.util.HashMap; import java.util.List; import java.util.Map; import net.sf.jasperreports.engine.JRExporterParameter; import net.sf.jasperreports.engine.JasperFillManager; import net.sf.jasperreports.engine.JasperPrint; import net.sf.jasperreports.engine.data.JRBeanCollectionDataSource; import net.sf.jasperreports.engine.export.JRTextExporter; import net.sf.jasperreports.engine.export.JRTextExporterParameter; public class JasperReportWithJavaBean &#123; public static void export() throws Exception&#123; InputStream inputStream = JasperReportWithJavaBean.class.getResourceAsStream(&quot;JavaBeanReport.jasper&quot;); Map&lt;Object,Object&gt; parameters = new HashMap&lt;Object,Object&gt;(); List&lt;User&gt; users = new ArrayList&lt;User&gt;(); users.add(new User(&quot;user_01&quot;,new Date())); users.add(new User(&quot;user_02&quot;,new Date())); users.add(new User(&quot;user_03&quot;,new Date())); JRBeanCollectionDataSource dataSource = new JRBeanCollectionDataSource(users); JasperPrint jasperPrint = JasperFillManager.fillReport(inputStream, parameters, dataSource); JRTextExporter exporter = new JRTextExporter(); exporter.setParameter(JRExporterParameter.JASPER_PRINT, jasperPrint); exporter.setParameter(JRExporterParameter.OUTPUT_FILE_NAME, &quot;javabean.txt&quot;); exporter.setParameter(JRTextExporterParameter.PAGE_WIDTH, 200); exporter.setParameter(JRTextExporterParameter.PAGE_HEIGHT, 100); exporter.exportReport(); &#125; public static void main(String[] args) throws Exception&#123; export(); &#125; &#125; 123456789101112131415161718192021222324252627&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;JasperReport&lt;/groupId&gt; &lt;artifactId&gt;JasperReport&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;net.sf.jasperreports&lt;/groupId&gt; &lt;artifactId&gt;jasperreports&lt;/artifactId&gt; &lt;version&gt;3.7.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.codehaus.groovy&lt;/groupId&gt; &lt;artifactId&gt;groovy-all&lt;/artifactId&gt; &lt;version&gt;1.7.5&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.13&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/project&gt;","categories":[{"name":"Dev","slug":"Dev","permalink":"http://www.philsblog.info/categories/Dev/"},{"name":"Java","slug":"Dev/Java","permalink":"http://www.philsblog.info/categories/Dev/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://www.philsblog.info/tags/Java/"}]},{"title":"Improve SQL Query Performance","slug":"Improve-SQL-Query-Performance","date":"2016-11-03T05:34:46.000Z","updated":"2020-04-26T11:37:18.921Z","comments":true,"path":"2016/11/03/Improve-SQL-Query-Performance.html","link":"","permalink":"http://www.philsblog.info/2016/11/03/Improve-SQL-Query-Performance.html","excerpt":"","text":"In general computer system, hard-drive read operations are about ten times more than write operation and in most cases, there are no performance issues with insert operation. The problems and bottlenecks are always detected in select queries. In this page, we summarize few different methods to improve SQL query performance. Avoid * in SELECT statement. Give the name of columns which you require. Avoid nchar and nvarchar if possible since both the data types takes just double memory as char and varchar. Avoid NULL in fixed-length field. In case of requirement of NULL, use variable-length (varchar) field that takes less space for NULL. Create Clustered and Non-Clustered Indexes. Most selective columns should be placed leftmost in the key of a non-clustered index. Better to create indexes on columns that have integer values instead of characters. Integer values use less overhead than character values. Use joins instead of sub-queries. Use Stored Procedure for frequently used data and more complex queries. Keep transaction as small as possible since transaction lock the processing tables data and may results into deadlocks. Avoid prefix “sp_” with user defined stored procedure name because SQL server first search the user defined procedure in the master database and after that in the current session database.","categories":[{"name":"Dev","slug":"Dev","permalink":"http://www.philsblog.info/categories/Dev/"},{"name":"SQL","slug":"Dev/SQL","permalink":"http://www.philsblog.info/categories/Dev/SQL/"}],"tags":[{"name":"SQL","slug":"SQL","permalink":"http://www.philsblog.info/tags/SQL/"}]},{"title":"Java Cache System","slug":"Java-Cache-System","date":"2016-10-13T05:24:59.000Z","updated":"2020-04-26T11:29:46.330Z","comments":true,"path":"2016/10/13/Java-Cache-System.html","link":"","permalink":"http://www.philsblog.info/2016/10/13/Java-Cache-System.html","excerpt":"","text":"JCS is a distributed caching system written in Java. It is intended to speed up applications by providing a means to manage cached data of various dynamic natures. Like any caching system, JCS is most useful for high read, low put applications. Latency times drop sharply and bottlenecks move away from the database in an effectively cached system. JCS 1.3 was the first official version. JCS 1.3 working on JDK 1.3+. JCS 2.0 working on JDK 1.6 and above. JCS 1.3 is stable, however it requires a lower version of JDK. JCS structure There are following jar files needed: commons-collections-2.1.1.jar commons-lang.2.3.jar commons-logging-1.0.4.jar concurrent-1.3.2.jar jcs-1.3.jar slf4j-api.jar It implements Serializable interface 12345678910111213public class UserInfo implements Serializable&#123; private String username; private String domain; public UserInfo(String name)&#123; this.username = name; &#125; public UserInfo(String name,String domain)&#123; this.username= name; this.domain =domain; &#125; &#125; Define Cache Class 12345678910111213141516171819202122232425262728293031323334353637383940 public class UserManager&#123; private JCS jcscache; private final String NAME_SPACE=\"userinfo\"; private static class UserManagerContainer&#123; private static UserManager instance = new UserManager(); &#125; public static UserManager getInstance()&#123; return UserManagerContainer.instance &#125; private UserManager()&#123; try&#123; jcscache= JCS.getInstance(NAME_SPACE); &#125; catch(CacheException e)&#123; &#125; &#125; public UserInfo get(String key)&#123; return (UserInfo) jcscache.get(key); &#125; pubilc void put(String key,UserInfo info,boolean isoverride)&#123; try&#123; if(isoverride)&#123; jcscache.put(key,info); &#125; else&#123; jcscache.putSafe(key,info); &#125; &#125; catch(CacheException e)&#123; &#125; &#125; &#125; JCS describe its functionality by configuration file, it’s easy to implement, all you need is change the values in configuration file.Configuration file: cache.ccf 12345678910111213jcs.default=DC jcs.defaultcacheattributes=org.engine.CompositeCacheAttributes jcs.defaultcacheattributes.MaxObjects=500000 jcs.defaultcacheattributes.MemoryCacheName=org.apache.jcs.engine.memory.lru.LRUMemoryCache jcs.defaultcacheattributes.UseMemoryShrinker=true jcs.defaultcacheattributes.MaxMemoryIdleTimeSeconds=1200 jcs.defaultcacheattributes.ShrinkerIntervalSeconds=30 jcs.defaultcacheattributes.MaxSpoolPerRun=500 jcs.default.elementattributes=org.apache.jcs.engine.ElementAttributes jcs.default.elementattributes.IsEternal=false jcs.auxiliary.DC=org.apache.jcs.auxiliary.disk.indexed.IndexedDiskCacheFactory jcs.auxiliary.DC.attributes=org.apache.jcs.auxiliary.disk.indexed.IndexedDiskCacheAttribute jcs.auxiliary.DC.attributes.DiskPatch=d:/memory","categories":[{"name":"Dev","slug":"Dev","permalink":"http://www.philsblog.info/categories/Dev/"},{"name":"Java","slug":"Dev/Java","permalink":"http://www.philsblog.info/categories/Dev/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://www.philsblog.info/tags/Java/"},{"name":"cache","slug":"cache","permalink":"http://www.philsblog.info/tags/cache/"}]},{"title":"Markdown all you need to know","slug":"Markdown-all-you-need-to-know","date":"2015-10-04T04:58:22.000Z","updated":"2020-04-26T11:40:17.698Z","comments":true,"path":"2015/10/04/Markdown-all-you-need-to-know.html","link":"","permalink":"http://www.philsblog.info/2015/10/04/Markdown-all-you-need-to-know.html","excerpt":"","text":"Here are lists of some of the most common use of Markdown. Headers123456# H1## H2### H3#### H4##### H5###### H6 H1H2H3H4H5H6Underline1--- three or more than three hyphens Superlink12345[linke name that would appear on the page](link address, &quot;title pop when hover&quot;)e.g.[phil&apos;s blog](http://www.philsblog.info, &quot;this is phil&apos;s blog&quot;)or you can use the link directly.&lt;http://www.philsblog.info&gt; linke name that would appear on the pagee.g.phil’s blogor you can use the link directly.http://www.philsblog.info Code referencing1use ``` at the begaining and end of your code block Highlight123*italic type***bold**~~delete~~ italic typebolddelete Image1![Alt text](https://media-cdn.tripadvisor.com/media/photo-s/0b/5d/38/4b/a-nice-small-water-fall.jpg &quot;Optional title&quot;) Lists12345678* Item* Item- Item- Item1. numbered - Mixed2. numbered * Mixed Item Item Item Item numbered Mixed numbered Mixed Blockquotes1234&gt; Quoted text.&gt; &gt; Quoted quote.&gt; * Quoted &gt; * List Quoted text. Quoted quote. Quoted List Preformatted123456Begin each line with two spaces or more to make text looke x a c t l y like you type it. Begin each line withtwo spaces or more tomake text looke x a c t l ylike you type it. Code1`This is code` This is code Code block1234~~~Code block is a bit tricky, since we are using code block to show you the syntaxyou can use three ~ surround your code~~~ 123```you can also use three ` for the same purpose``` Syntax highlighting123456```css#button &#123; border: none;&#125;``` 123#button &#123; border: none;&#125; Table12345| Tables | Are | Cool ||----------|:-------------:|------:|| col 1 is | left-aligned | $1600 || col 2 is | centered | $12 || col 3 is | right-aligned | $1 | Tables Are Cool col 1 is left-aligned $1600 col 2 is centered $12 col 3 is right-aligned $1","categories":[{"name":"Tools","slug":"Tools","permalink":"http://www.philsblog.info/categories/Tools/"}],"tags":[{"name":"Tools","slug":"Tools","permalink":"http://www.philsblog.info/tags/Tools/"},{"name":"markdown","slug":"markdown","permalink":"http://www.philsblog.info/tags/markdown/"}]},{"title":"Financial Management Group","slug":"Financial-Management-Group","date":"2015-06-27T07:05:36.000Z","updated":"2020-04-26T10:44:16.421Z","comments":true,"path":"2015/06/27/Financial-Management-Group.html","link":"","permalink":"http://www.philsblog.info/2015/06/27/Financial-Management-Group.html","excerpt":"","text":"J2EE developerMelbourne, VIC 2015 - 2016 Description: BCM is a business coach model system that helps investors in making decisions and learning how to make a portfolio. The system provides users with a wide range of companies and share’s information including analyzed history data. Based on a specially designed weighted-evaluation method, users can easily and quickly learn how to make investments. Responsibilities: Utilized the various J2EE design patterns like Front View Controller, Business Delegate, Factory , Singleton and DAO to develop the business modules based on the required functionality. Designed and maintained the financial system on Glassfish. Analyzed and processed raw input data from source company. Developed visualized and responsive web pages using amChart.js, dataTables, jQuery, bootstrap. Maintaining and building mySQL database by using phpMyAdmin. Developed custom JSF Components. Used JavaScript for Client Side validations. Involved in writing AJAX scripts for the requests to process quickly. Used Spring IOC to inject Services and their dependencies. Implemented Stateless Session Beans to implement the business logic as a service. Used SVN as repository for all Project related documents. Used Log4j package for the debugging, info and error tracings. BCMFinancial Management GroupEnvironment: Core Java, J2EE, J2SE 5.0, Servlets, Spring, JPA, JNDI, mySql, JSF 2.0, Ajax, SVN, NetBeans and Log4j.","categories":[{"name":"Work history","slug":"Work-history","permalink":"http://www.philsblog.info/categories/Work-history/"},{"name":"Financial Management Group","slug":"Work-history/Financial-Management-Group","permalink":"http://www.philsblog.info/categories/Work-history/Financial-Management-Group/"}],"tags":[{"name":"Work history","slug":"Work-history","permalink":"http://www.philsblog.info/tags/Work-history/"}]},{"title":"Hello World","slug":"hello-world","date":"2013-09-04T02:58:22.000Z","updated":"2020-04-26T08:11:35.110Z","comments":true,"path":"2013/09/04/hello-world.html","link":"","permalink":"http://www.philsblog.info/2013/09/04/hello-world.html","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[]},{"title":"State Grid Fiberlink Inc.","slug":"State-Grid-Fiberlink-Corporation","date":"2012-11-13T06:10:06.000Z","updated":"2020-04-26T10:44:01.752Z","comments":true,"path":"2012/11/13/State-Grid-Fiberlink-Corporation.html","link":"","permalink":"http://www.philsblog.info/2012/11/13/State-Grid-Fiberlink-Corporation.html","excerpt":"","text":"Junior Java DeveloperBeijing, China, 2011 - 2013 Description: Smart Home is state-funded home automation system. Consumers could easily connect and control all the household appliances through Smart sockets and their smart phones. It is also convenient to manage the consumption of electrical power and switch the electricity plans through the system.Responsibilities: Developed and published apps for the Android platform using Android SDK, JavaSE 6, eclipse IDE and the Android AVD. Participated in a program of weekly seminar about mobile devices, with emphasis on the Android platform. Developed Web applications using Core Java, Java Script, MVC, OOD, OOP, and client side validation using java Script, business service layer using MVC. Research and Development of REST services and migration of existing SOAP services to REST services along with implementation of new services in REST. Also participated in several Linux technical training classes, including an Introduction to the Linux Kernel for Application Engineers, Advanced Linux Kernel and Device Drivers for Linux (prepare for Smart Socket and Smart Router development). Writing efficient, maintainable and reusable code that preserves privacy and security. Proficient in object-oriented design, data structures, problem solving, complexity analysis, and debugging. Regarded as a self-motivated and well organized team player. Smart Home for AndroidFiberlink Environment: Core Java, J2EE, ADT, Android SDK 2.2, JavaSE 6, RESTful web service, Spring, JPA, JNDI, oracle sql, SVN, Eclipse.","categories":[{"name":"Work history","slug":"Work-history","permalink":"http://www.philsblog.info/categories/Work-history/"},{"name":"State Grid Fiberlink Inc.","slug":"Work-history/State-Grid-Fiberlink-Inc","permalink":"http://www.philsblog.info/categories/Work-history/State-Grid-Fiberlink-Inc/"}],"tags":[{"name":"Work history","slug":"Work-history","permalink":"http://www.philsblog.info/tags/Work-history/"}]}]}